#ä¸­æ–‡æ³¨é‡Šä¼˜åŒ–ç‰ˆ20250528_DDP
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import math
import json
from datetime import datetime
from torch.amp import autocast, GradScaler

# DDPç›¸å…³å¯¼å…¥
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# --- DDP è¾…åŠ©å‡½æ•° ---
def setup_ddp(rank, world_size):
    """åˆå§‹åŒ–DDPè¿›ç¨‹ç»„"""
    os.environ['MASTER_ADDR'] = 'localhost' # é€šå¸¸ç”±torchrunè®¾ç½®ï¼Œè¿™é‡Œä¸ºæœ¬åœ°æµ‹è¯•æä¾›
    os.environ['MASTER_PORT'] = '12355'     # é€‰æ‹©ä¸€ä¸ªæœªè¢«å ç”¨çš„ç«¯å£
    
    # æ ¹æ®åç«¯å’Œç¯å¢ƒè°ƒæ•´
    backend = 'nccl' if torch.cuda.is_available() else 'gloo'
    dist.init_process_group(backend, rank=rank, world_size=world_size)
    if torch.cuda.is_available():
        torch.cuda.set_device(rank) # local_rank is implicitly rank here
    print(f"DDP: Rank {rank}/{world_size} initialized using backend {backend}.")

def cleanup_ddp():
    """æ¸…ç†DDPè¿›ç¨‹ç»„"""
    if dist.is_initialized():
        dist.destroy_process_group()
        print("DDP: Process group destroyed.")

def is_main_process(rank):
    """æ£€æŸ¥å½“å‰æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ (rank 0)"""
    return rank == 0

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
def set_seed(seed=42, rank=0):
    torch.manual_seed(seed + rank) # ä¸ºä¸åŒè¿›ç¨‹è®¾ç½®ç•¥å¾®ä¸åŒçš„ç§å­ï¼Œä½†åŸºäºåŒä¸€åŸºç¡€
    np.random.seed(seed + rank)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed + rank)
        # ä¸ºäº†CUDNNçš„ç¡®å®šæ€§è¡Œä¸º
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


# --- è¾…åŠ©å‡½æ•°å’Œå±‚ï¼Œç”¨äºå®ç°StyleGAN3å¯å‘å¼çš„æŠ—æ··å æ“ä½œ ---

def get_blur_kernel_2d(size=3, normalize=True, dtype=torch.float32, device='cpu'):
    """
    ç”Ÿæˆä¸€ä¸ª2Dæ¨¡ç³Šæ ¸ (ä¾‹å¦‚ï¼Œè¿‘ä¼¼äºStyleGANä¸­ä½¿ç”¨çš„FIRæ»¤æ³¢å™¨)ã€‚
    è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„ã€å¯åˆ†ç¦»çš„äºŒé¡¹å¼æ»¤æ³¢å™¨ã€‚
    è¿”å›: torch.Tensor, shape [size, size]
    """
    k_1d_list = []
    if size == 1:
        k_1d_list = [1.]
    elif size == 3:
        k_1d_list = [1., 2., 1.]
    elif size == 5:
        k_1d_list = [1., 4., 6., 4., 1.]
    else: 
        print(f"è­¦å‘Š: ä¸æ”¯æŒçš„æ¨¡ç³Šæ ¸å¤§å° {size}, ä½¿ç”¨3x3æ›¿ä»£ã€‚")
        k_1d_list = [1., 2., 1.]
    
    k_1d = torch.tensor(k_1d_list, dtype=dtype, device=device)
    kernel_2d = torch.outer(k_1d, k_1d)
    if normalize:
        kernel_2d = kernel_2d / torch.sum(kernel_2d)
    return kernel_2d

class Blur(nn.Module):
    """
    ä¸€ä¸ªç®€å•çš„æ¨¡ç³Šå±‚ï¼Œç”¨äºåœ¨ç‰¹å¾å›¾ä¸Šåº”ç”¨2Dæ¨¡ç³Šã€‚
    """
    def __init__(self, channels, kernel_size=3):
        super().__init__()
        self.channels = channels
        self.kernel_size = kernel_size
        self.padding = kernel_size // 2

        blur_k_2d = get_blur_kernel_2d(kernel_size, normalize=True, dtype=torch.float32)
        kernel = blur_k_2d.reshape(1, 1, kernel_size, kernel_size).repeat(channels, 1, 1, 1)
        self.register_buffer('blur_kernel', kernel, persistent=False)


    def forward(self, x):
        kernel_to_use = None 
        if x.shape[1] != self.channels:
            # åŠ¨æ€é€‚åº”è¾“å…¥é€šé“æ•°ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹æœ‰ç”¨ï¼Œä½†è¦ç¡®ä¿åˆ†ç»„å·ç§¯çš„ç»„æ•°æ­£ç¡®
            # å¯¹äºDDPï¼Œå¦‚æœæ¨¡å‹ç»“æ„åœ¨ä¸åŒrankä¸Šå› è¿™ä¸ªé€»è¾‘äº§ç”Ÿå·®å¼‚ï¼Œå¯èƒ½ä¼šæœ‰é—®é¢˜ã€‚
            # ä½†è¿™é‡Œæ˜¯æ ¹æ®è¾“å…¥xçš„shape[1]æ¥repeatï¼Œåº”è¯¥æ˜¯å®‰å…¨çš„ã€‚
            blur_k_2d_dyn = get_blur_kernel_2d(self.kernel_size, normalize=True, dtype=x.dtype, device=x.device)
            kernel_to_use = blur_k_2d_dyn.reshape(1, 1, self.kernel_size, self.kernel_size).repeat(x.shape[1], 1, 1, 1)
            return F.conv2d(x, kernel_to_use, padding=self.padding, groups=x.shape[1])
        
        kernel_to_use = self.blur_kernel.to(device=x.device, dtype=x.dtype)
        return F.conv2d(x, kernel_to_use, padding=self.padding, groups=self.channels)

class UpsampleBlock(nn.Module):
    """
    ä¸Šé‡‡æ ·å—ï¼Œå€Ÿé‰´StyleGAN3çš„æŠ—æ··å æ€æƒ³ã€‚
    æ“ä½œé¡ºåº: æ’å€¼ä¸Šé‡‡æ · -> è½»å¾®æ¨¡ç³Š (æŠ—æ··å ) -> å·ç§¯ -> å½’ä¸€åŒ– -> æ¿€æ´»
    """
    def __init__(self, in_channels, out_channels, conv_kernel_size=3, conv_padding=1,
                 blur_kernel_size=3, use_bias=False):
        super().__init__()
        self.interpolate_mode = 'bilinear' # StyleGAN3 æ¨è 'bilinear' æˆ–æ›´å¤æ‚çš„æ’å€¼
        self.blur = None

        self.apply_blur = blur_kernel_size > 0
        if self.apply_blur:
            self.blur = Blur(in_channels, kernel_size=blur_kernel_size)
        
        self.conv = nn.Conv2d(in_channels, out_channels, conv_kernel_size, stride=1, padding=conv_padding, bias=use_bias)
        # ä½¿ç”¨ SyncBatchNorm æ›¿æ¢ BatchNorm2d ä»¥é€‚åº” DDP
        # æ³¨æ„ï¼šSyncBatchNorm é€šå¸¸åœ¨æ¨¡å‹è¾ƒå¤§ï¼Œbatch_sizeè¾ƒå°æ—¶è¡¨ç°æ›´å¥½ï¼Œä½†ä¹Ÿå¯èƒ½å¢åŠ é€šä¿¡å¼€é”€
        # å¦‚æœæ€§èƒ½ä¸‹é™æˆ–é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘æ˜¯å¦æ‰€æœ‰BNéƒ½éœ€è¦Syncï¼Œæˆ–è€…ç‰¹å®šæ¡ä»¶ä¸‹åˆ‡æ¢å›æ™®é€šBN
        self.norm = nn.SyncBatchNorm(out_channels) if dist.is_initialized() and dist.get_world_size() > 1 else nn.BatchNorm2d(out_channels)
        self.activation = nn.ReLU(inplace=True)

    def forward(self, x):
        x_upsampled = F.interpolate(x, scale_factor=2, mode=self.interpolate_mode, align_corners=False)
        x_blurred = x_upsampled
        if self.apply_blur and self.blur is not None:
            x_blurred = self.blur(x_upsampled)
        
        x_conv = self.conv(x_blurred)
        x_norm = self.norm(x_conv)
        x_activated = self.activation(x_norm)
        return x_activated

class DownsampleBlock(nn.Module):
    """
    ä¸‹é‡‡æ ·å—ï¼Œå€Ÿé‰´StyleGAN3çš„æŠ—æ··å æ€æƒ³ã€‚
    æ“ä½œé¡ºåº: è½»å¾®æ¨¡ç³Š (æŠ—æ··å ) -> å·ç§¯ (stride=2å®ç°ä¸‹é‡‡æ ·) -> å½’ä¸€åŒ– (å¯é€‰) -> æ¿€æ´»
    """
    def __init__(self, in_channels, out_channels, conv_kernel_size=3, conv_padding=1,
                 blur_kernel_size=3, use_bias=False, use_norm=True):
        super().__init__()
        self.blur = None
        
        self.apply_blur = blur_kernel_size > 0
        if self.apply_blur:
            self.blur = Blur(in_channels, kernel_size=blur_kernel_size)

        self.conv = nn.Conv2d(in_channels, out_channels, conv_kernel_size, stride=2, padding=conv_padding, bias=use_bias)
        
        self.use_norm = use_norm
        self.norm = None
        if self.use_norm:
            self.norm = nn.SyncBatchNorm(out_channels) if dist.is_initialized() and dist.get_world_size() > 1 else nn.BatchNorm2d(out_channels)
        self.activation = nn.LeakyReLU(0.2, inplace=True)

    def forward(self, x):
        x_blurred = x
        if self.apply_blur and self.blur is not None:
            x_blurred = self.blur(x)
        
        x_conv = self.conv(x_blurred)
        x_norm = x_conv
        if self.use_norm and self.norm is not None:
            x_norm = self.norm(x_conv)
        
        x_activated = self.activation(x_norm)
        return x_activated

# --- æ•°æ®é›†å®šä¹‰ ---
class ProductImageDataset(Dataset):
    def __init__(self, data_dir, transform=None, rank=0): # rank for logging if needed
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = []
        if os.path.exists(data_dir) and os.path.isdir(data_dir):
            self.image_files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
            if is_main_process(rank): # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°ï¼Œé¿å…DDPä¸‹é‡å¤æ‰“å°
                print(f"Rank {rank}: æ‰¾åˆ° {len(self.image_files)} å¼ å›¾ç‰‡åœ¨ {data_dir}")
        else:
            if is_main_process(rank):
                print(f"Rank {rank}: é”™è¯¯: æ•°æ®ç›®å½• {data_dir} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•ã€‚")
        
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.data_dir, self.image_files[idx])
        image = None
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            # åœ¨DDPä¸­ï¼Œä¸€ä¸ªè¿›ç¨‹å‡ºé”™å¯èƒ½å¯¼è‡´æŒ‚èµ·ï¼Œé”™è¯¯å¤„ç†è¦å°å¿ƒ
            # print(f"é”™è¯¯: æ— æ³•æ‰“å¼€æˆ–è½¬æ¢å›¾ç‰‡ {img_path}: {e}") # å‡å°‘æ‰“å°
            placeholder_size = (256, 256) 
            if self.transform:
                 for t_item in self.transform.transforms:
                     if isinstance(t_item, transforms.Resize):
                         if isinstance(t_item.size, int):
                             placeholder_size = (t_item.size, t_item.size)
                         else:
                             placeholder_size = t_item.size 
                         break
            image = Image.new('RGB', placeholder_size, color='black') 
            if self.transform:
                return self.transform(image)
            else: 
                return transforms.ToTensor()(image)

        if self.transform and image is not None:
            image = self.transform(image)
        elif image is None: 
             placeholder_size_fallback = (256,256)
             if self.transform:
                  for t_item in self.transform.transforms:
                     if isinstance(t_item, transforms.Resize):
                         if isinstance(t_item.size, int):
                             placeholder_size_fallback = (t_item.size, t_item.size)
                         else:
                             placeholder_size_fallback = t_item.size
                         break
             image_fallback = Image.new('RGB', placeholder_size_fallback, color='grey')
             if self.transform:
                 return self.transform(image_fallback)
             else:
                 return transforms.ToTensor()(image_fallback)
        return image

# --- æ¨¡å‹å®šä¹‰ ---
class Generator(nn.Module):
    def __init__(self, nz=100, ngf=64, nc=3, blur_kernel_size=3):
        super(Generator, self).__init__()
        self.nz = nz
        self.ngf = ngf
        self.nc = nc
        
        self.initial = nn.Sequential(
            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
            # BatchNorm -> SyncBatchNorm if DDP
            nn.SyncBatchNorm(ngf * 8) if dist.is_initialized() and dist.get_world_size() > 1 else nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True)
        )
        
        self.up1 = UpsampleBlock(ngf * 8, ngf * 4, blur_kernel_size=blur_kernel_size)
        self.up2 = UpsampleBlock(ngf * 4, ngf * 2, blur_kernel_size=blur_kernel_size)
        self.up3 = UpsampleBlock(ngf * 2, ngf, blur_kernel_size=blur_kernel_size)
        # æ ¹æ®ç›®æ ‡è¾“å‡ºå°ºå¯¸åŠ¨æ€æ·»åŠ æ›´å¤šä¸Šé‡‡æ ·å±‚ï¼Œæ”¯æŒåˆ°512x512
        self.up4 = UpsampleBlock(ngf, ngf, blur_kernel_size=blur_kernel_size) # to 64x64 (feature map size)
        self.up5 = UpsampleBlock(ngf, ngf, blur_kernel_size=blur_kernel_size) # to 128x128
        self.up6 = UpsampleBlock(ngf, ngf, blur_kernel_size=blur_kernel_size) # to 256x256
        self.up7 = UpsampleBlock(ngf, ngf, blur_kernel_size=blur_kernel_size) # to 512x512
        
        # è¾“å‡ºå±‚ï¼Œä¸ä½¿ç”¨BNï¼Œé€šå¸¸åœ¨GANçš„Gçš„æœ€åä¸€å±‚ä½¿ç”¨Tanh
        self.output_64 = nn.Conv2d(ngf, nc, kernel_size=3, stride=1, padding=1)
        self.output_128 = nn.Conv2d(ngf, nc, kernel_size=3, stride=1, padding=1)
        self.output_256 = nn.Conv2d(ngf, nc, kernel_size=3, stride=1, padding=1)
        self.output_512 = nn.Conv2d(ngf, nc, kernel_size=3, stride=1, padding=1)
        
    def forward(self, input_noise, target_size=512):
        x = input_noise.view(-1, self.nz, 1, 1) # Reshape noise
        
        x = self.initial(x) # To 4x4
        x = self.up1(x) # To 8x8
        x = self.up2(x) # To 16x16
        x = self.up3(x) # To 32x32 (features for 64x64 output)
        
        feat_64 = self.up4(x) # To 64x64 (features for 128x128 output, or output for 64x64)
        if target_size == 64:
            return torch.tanh(self.output_64(feat_64))
        
        feat_128 = self.up5(feat_64) # To 128x128
        if target_size == 128:
            return torch.tanh(self.output_128(feat_128))
            
        feat_256 = self.up6(feat_128) # To 256x256
        if target_size == 256:
            return torch.tanh(self.output_256(feat_256))
            
        feat_512 = self.up7(feat_256) # To 512x512
        return torch.tanh(self.output_512(feat_512))

class Discriminator(nn.Module):
    def __init__(self, nc=3, ndf=64, blur_kernel_size=3):
        super(Discriminator, self).__init__()
        self.nc = nc
        self.ndf = ndf
        
        # åˆ¤åˆ«å™¨çš„ç¬¬ä¸€å±‚é€šå¸¸ä¸ä½¿ç”¨BN
        self.down1 = DownsampleBlock(nc, ndf, blur_kernel_size=blur_kernel_size, use_norm=False)
        self.down2 = DownsampleBlock(ndf, ndf * 2, blur_kernel_size=blur_kernel_size)
        self.down3 = DownsampleBlock(ndf * 2, ndf * 4, blur_kernel_size=blur_kernel_size)
        self.down4 = DownsampleBlock(ndf * 4, ndf * 8, blur_kernel_size=blur_kernel_size)
        # æ ¹æ®è¾“å…¥å›¾åƒå¤§å°åŠ¨æ€è°ƒæ•´å±‚æ•°
        self.down5 = DownsampleBlock(ndf * 8, ndf * 8, blur_kernel_size=blur_kernel_size) # For inputs >= 128x128 -> feature map to 4x4 (if from 128) or 8x8 (if from 256)
        self.down6 = DownsampleBlock(ndf * 8, ndf * 8, blur_kernel_size=blur_kernel_size) # For inputs >= 256x256 -> feature map to 4x4 (if from 256) or 8x8 (if from 512)

        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4)) # Ensure final feature map is 4x4 before final conv
        self.final_conv = nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=0, bias=False)
        
    def forward(self, input_image):
        # input_image: [N, C, H, W]
        x = self.down1(input_image) # H/2
        x = self.down2(x)           # H/4
        x = self.down3(x)           # H/8
        x = self.down4(x)           # H/16

        # Dynamically use more downsampling layers based on feature map size
        # Assumes H_initial (input_image.shape[2]) is one of [64, 128, 256, 512]
        # After down4, H is H_initial / 16.
        # If H_initial = 64, H_after_down4 = 4.
        # If H_initial = 128, H_after_down4 = 8.
        # If H_initial = 256, H_after_down4 = 16.
        # If H_initial = 512, H_after_down4 = 32.
        
        if x.shape[2] > 8: # Corresponds to input_image > 128x128 (e.g. 256 or 512)
             x = self.down5(x) # H/32
        if x.shape[2] > 4 : # Corresponds to input_image > 64x64 (e.g. 128, 256 or 512, after down5 if applicable)
             x = self.down6(x) # H/64 (if from 256/512) or H/32 (if from 128)
        
        x = self.adaptive_pool(x) # Guarantees 4x4 feature map
        x = self.final_conv(x)    # Output is [N, 1, 1, 1]
        
        return x.view(-1) # Flatten to [N] for BCEWithLogitsLoss

# --- æƒé‡åˆå§‹åŒ– ---
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if hasattr(m, 'bias') and m.bias is not None:
            nn.init.constant_(m.bias.data, 0.0)
    elif classname.find('BatchNorm') != -1 or classname.find('SyncBatchNorm') != -1: # Include SyncBatchNorm
        if hasattr(m, 'weight') and m.weight is not None:
             nn.init.normal_(m.weight.data, 1.0, 0.02)
        if hasattr(m, 'bias') and m.bias is not None:
            nn.init.constant_(m.bias.data, 0.0)

# --- æ•°æ®åŠ è½½å™¨åˆ›å»º ---
def create_data_loaders(data_dir, batch_size=8, num_workers=0, rank=0, world_size=1):
    transforms_dict = {}
    datasets_dict = {}
    dataloaders_dict = {}
    samplers_dict = {}

    for size_val in [64, 128, 256, 512]:
        current_transform = transforms.Compose([
            transforms.Resize((size_val, size_val)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])
        transforms_dict[size_val] = current_transform
        
        current_dataset = ProductImageDataset(data_dir, transforms_dict[size_val], rank=rank)
        datasets_dict[size_val] = current_dataset
        
        sampler = None
        shuffle_dl = True # Shuffle for non-DDP or if sampler handles it
        if world_size > 1: # DDP active
            sampler = DistributedSampler(current_dataset, num_replicas=world_size, rank=rank, shuffle=True)
            samplers_dict[size_val] = sampler
            shuffle_dl = False # Sampler handles shuffling

        if len(current_dataset) > 0:
            # drop_last is important for DDP if batch sizes are not perfectly divisible
            # It's also generally good for GANs to have consistent batch sizes
            effective_len = len(current_dataset) // world_size if world_size > 0 else len(current_dataset)
            if effective_len >= batch_size : # batch_size is per-GPU here
                dataloaders_dict[size_val] = DataLoader(current_dataset, batch_size=batch_size, 
                                                    shuffle=shuffle_dl, sampler=sampler,
                                                    num_workers=num_workers, drop_last=True, 
                                                    pin_memory=torch.cuda.is_available())
            else:
                if is_main_process(rank):
                    print(f"è­¦å‘Š: {size_val}x{size_val} æ•°æ®é›†åœ¨rank {rank}ä¸Šçš„æœ‰æ•ˆæ ·æœ¬æ•° {effective_len} å°äºæ‰¹å¤§å° {batch_size} (ä¸”drop_last=True)ï¼Œæ— æ³•åˆ›å»ºDataLoaderã€‚")
                dataloaders_dict[size_val] = None
        else:
            if is_main_process(rank):
                print(f"è­¦å‘Š: {size_val}x{size_val} æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºDataLoaderã€‚")
            dataloaders_dict[size_val] = None
            
    return dataloaders_dict

# --- ä¿å­˜æ ·æœ¬å›¾ç‰‡ (åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ) ---
def save_sample_images(generator_module, epoch, current_size, nz=100, save_dir_prefix='generated_samples', device='cpu', amp_enabled=False):
    # generator_module should be netG.module if DDP, or netG if not
    generator_module.eval() 
    with torch.no_grad(): 
        fixed_noise = torch.randn(16, nz, device=device) 
        amp_dtype = torch.float16 if device.type == 'cuda' and amp_enabled else torch.float32

        with autocast(device_type=device.type, enabled=amp_enabled, dtype=amp_dtype):
            fake_images = generator_module(fixed_noise, target_size=current_size)
        
        fake_images = (fake_images + 1) / 2.0 
        fake_images = torch.clamp(fake_images, 0, 1)
        
        save_dir = f'{save_dir_prefix}_{current_size}x{current_size}'
        os.makedirs(save_dir, exist_ok=True)
        
        fake_images_np = fake_images.cpu().float().numpy() 
        
        fig, axes = plt.subplots(4, 4, figsize=(8, 8))
        fig.suptitle(f'Epoch {epoch} - Size {current_size}x{current_size}', fontsize=16)
        for i_plt in range(16): # Renamed i to i_plt to avoid conflict
            row, col = i_plt // 4, i_plt % 4
            img_np_permuted = np.transpose(fake_images_np[i_plt], (1, 2, 0))
            axes[row, col].imshow(img_np_permuted)
            axes[row, col].axis('off')
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        save_path = os.path.join(save_dir, f'epoch_{epoch:04d}.png')
        plt.savefig(save_path, dpi=150)
        plt.close(fig) 
    generator_module.train()

# --- æ£€æŸ¥ç‚¹ä¿å­˜ä¸åŠ è½½ (åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œä¿å­˜ï¼Œæ‰€æœ‰è¿›ç¨‹åŠ è½½) ---
def save_checkpoint(netG_module, netD_module, optimizerG, optimizerD, scaler, epoch, phase_info, 
                    checkpoint_dir='checkpoints', amp_enabled=False, rank=0):
    # netG_module and netD_module should be the underlying models (e.g., netG.module)
    if not is_main_process(rank): # åªæœ‰ä¸»è¿›ç¨‹ä¿å­˜æ£€æŸ¥ç‚¹
        return

    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint_data = {
        'generator_state_dict': netG_module.state_dict(),
        'discriminator_state_dict': netD_module.state_dict(),
        'optimizerG_state_dict': optimizerG.state_dict(),
        'optimizerD_state_dict': optimizerD.state_dict(),
        'epoch': epoch, # Global epoch
        'phase_info': phase_info, # Info about current training phase
        'timestamp': datetime.now().isoformat()
    }
    if amp_enabled and scaler is not None:
        checkpoint_data['scaler_state_dict'] = scaler.state_dict()
    
    # æ–°æ£€æŸ¥ç‚¹è·¯å¾„
    new_checkpoint_filename = f'checkpoint_epoch_{epoch:04d}.pth'
    new_checkpoint_path = os.path.join(checkpoint_dir, new_checkpoint_filename)
    torch.save(checkpoint_data, new_checkpoint_path)
    
    # æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°çš„
    latest_json_path = os.path.join(checkpoint_dir, 'latest.json')
    previous_checkpoint_file_to_delete = None

    if os.path.exists(latest_json_path):
        try:
            with open(latest_json_path, 'r', encoding='utf-8') as f:
                latest_info_old = json.load(f)
            if 'latest_checkpoint' in latest_info_old:
                # Make sure it's a .pth file as expected
                if os.path.basename(latest_info_old['latest_checkpoint']).startswith('checkpoint_epoch_') and \
                   latest_info_old['latest_checkpoint'].endswith('.pth'):
                    previous_checkpoint_file_to_delete = latest_info_old['latest_checkpoint']
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"Rank {rank}: è­¦å‘Š: è¯»å–æˆ–è§£ææ—§çš„ {latest_json_path} å¤±è´¥ã€‚å¯èƒ½æ— æ³•æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ã€‚")

    # æ›´æ–° latest.json æŒ‡å‘æ–°çš„æ£€æŸ¥ç‚¹
    latest_info_new = {
        'latest_checkpoint': new_checkpoint_path, # Store the full path
        'epoch': epoch,
        'phase_info': phase_info,
        'timestamp': datetime.now().isoformat()
    }
    with open(latest_json_path, 'w', encoding='utf-8') as f:
        json.dump(latest_info_new, f, ensure_ascii=False, indent=2)
    
    print(f"Rank {rank}: æ£€æŸ¥ç‚¹å·²ä¿å­˜: {new_checkpoint_path}")

    # åˆ é™¤æ—§çš„ .pth æ–‡ä»¶ (å¦‚æœå®ƒå­˜åœ¨ä¸”ä¸æ–°æ–‡ä»¶ä¸åŒ)
    if previous_checkpoint_file_to_delete and \
       previous_checkpoint_file_to_delete != new_checkpoint_path and \
       os.path.exists(previous_checkpoint_file_to_delete):
        try:
            os.remove(previous_checkpoint_file_to_delete)
            print(f"Rank {rank}: å·²åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {previous_checkpoint_file_to_delete}")
        except OSError as e:
            print(f"Rank {rank}: è­¦å‘Š: åˆ é™¤æ—§æ£€æŸ¥ç‚¹æ–‡ä»¶ {previous_checkpoint_file_to_delete} å¤±è´¥: {e}")


def load_checkpoint(checkpoint_path, netG_module, netD_module, optimizerG, optimizerD, scaler, device, rank=0): 
    # netG_module and netD_module are the underlying models
    if not os.path.exists(checkpoint_path):
        if is_main_process(rank):
            print(f"Rank {rank}: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return 0, {}
    
    # æ‰€æœ‰è¿›ç¨‹éƒ½ä»åŒä¸€ä¸ªæ–‡ä»¶åŠ è½½ï¼Œç¡®ä¿map_locationæ­£ç¡®
    checkpoint_data = torch.load(checkpoint_path, map_location=device) 
    
    netG_module.load_state_dict(checkpoint_data['generator_state_dict'])
    netD_module.load_state_dict(checkpoint_data['discriminator_state_dict'])
    
    if optimizerG and 'optimizerG_state_dict' in checkpoint_data:
        optimizerG.load_state_dict(checkpoint_data['optimizerG_state_dict'])
    if optimizerD and 'optimizerD_state_dict' in checkpoint_data:
        optimizerD.load_state_dict(checkpoint_data['optimizerD_state_dict'])
    
    if scaler is not None and 'scaler_state_dict' in checkpoint_data: 
        scaler.load_state_dict(checkpoint_data['scaler_state_dict'])
        if is_main_process(rank): print("Rank {rank}: GradScalerçŠ¶æ€å·²åŠ è½½ã€‚")
    elif scaler is not None and is_main_process(rank):
        print("Rank {rank}: è­¦å‘Š: æ£€æŸ¥ç‚¹ä¸­æœªæ‰¾åˆ°GradScalerçŠ¶æ€ã€‚å°†ä½¿ç”¨æ–°çš„Scalerã€‚")

    start_epoch = checkpoint_data.get('epoch', 0) # Global epoch completed
    phase_info = checkpoint_data.get('phase_info', {})
    
    if is_main_process(rank):
        print(f"Rank {rank}: å·²ä»æ£€æŸ¥ç‚¹åŠ è½½: {checkpoint_path}")
        print(f"Rank {rank}: æ¢å¤åˆ°å…¨å±€ç¬¬ {start_epoch} ä¸ªepochä¹‹åï¼Œé˜¶æ®µä¿¡æ¯: {phase_info}")
    
    return start_epoch, phase_info


# --- ä¸»è®­ç»ƒå‡½æ•° ---
def train_gan(data_dir, num_epochs_override=None, batch_size=4, lr=0.0002, nz=100, 
              resume_from_checkpoint=False, blur_kernel_size=3, num_workers=0, grad_clip_norm=1.0,
              rank=0, world_size=1, ddp_active=False):
    
    # DDP: device is the local rank's GPU
    device = torch.device(f"cuda:{rank}") if torch.cuda.is_available() and ddp_active else \
             torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    amp_enabled = (device.type == 'cuda') 
    amp_dtype = torch.float16 if amp_enabled else torch.float32
    
    if is_main_process(rank):
        print(f"Rank {rank}: ä½¿ç”¨è®¾å¤‡: {device}")
        if amp_enabled:
            print(f"Rank {rank}: è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP) å·²å¯ç”¨ï¼Œä½¿ç”¨æ•°æ®ç±»å‹: {amp_dtype}ã€‚")
        else:
            print("Rank {rank}: è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP) æœªå¯ç”¨ã€‚")

    # æ¨¡å‹åˆå§‹åŒ–åœ¨å„è‡ªçš„è®¾å¤‡ä¸Š
    netG_base = Generator(nz=nz, ngf=64, nc=3, blur_kernel_size=blur_kernel_size).to(device)
    netD_base = Discriminator(nc=3, ndf=64, blur_kernel_size=blur_kernel_size).to(device)
    
    netG_base.apply(weights_init)
    netD_base.apply(weights_init)

    # DDP åŒ…è£…
    if ddp_active:
        # find_unused_parameters=True can be important for GANs if parts of graph aren't always used
        netG = DDP(netG_base, device_ids=[rank] if device.type == 'cuda' else None, output_device=rank if device.type == 'cuda' else None, find_unused_parameters=True)
        netD = DDP(netD_base, device_ids=[rank] if device.type == 'cuda' else None, output_device=rank if device.type == 'cuda' else None, find_unused_parameters=True)
    else:
        netG = netG_base
        netD = netD_base
    
    # è·å–ç”¨äºä¿å­˜/åŠ è½½/è¯„ä¼°çš„æ¨¡å—
    netG_module = netG.module if ddp_active else netG
    netD_module = netD.module if ddp_active else netD

    criterion = nn.BCEWithLogitsLoss()
    
    optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999), eps=1e-8)
    optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999), eps=1e-8)

    scaler = GradScaler(enabled=amp_enabled)
    
    dataloaders = create_data_loaders(data_dir, batch_size, num_workers, rank, world_size)
    if not any(dataloaders.values()):
        if is_main_process(rank):
            print("Rank {rank}: é”™è¯¯: æ‰€æœ‰å°ºå¯¸çš„æ•°æ®åŠ è½½å™¨å‡ä¸ºç©ºã€‚ç¨‹åºå°†é€€å‡ºã€‚")
        return None, None

    training_phases = [
        {'size': 64, 'epochs': 10}, 
        {'size': 128, 'epochs': 10},
        {'size': 256, 'epochs': 10},
        {'size': 512, 'epochs': 10}
    ]
    if num_epochs_override is not None:
        for phase_item in training_phases:
            phase_item['epochs'] = num_epochs_override

    # è®¡ç®—æœ‰æ•ˆçš„æ€»epochæ•° (åªè®¡ç®—æœ‰æ•°æ®åŠ è½½å™¨çš„é˜¶æ®µ)
    total_epochs_across_phases = sum(p['epochs'] for p in training_phases if dataloaders.get(p['size']) is not None)
    if total_epochs_across_phases == 0 :
        if is_main_process(rank):
            print(f"Rank {rank}: é”™è¯¯: æ²¡æœ‰å¯è®­ç»ƒçš„é˜¶æ®µã€‚ç¨‹åºå°†é€€å‡ºã€‚")
        return None, None

    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max=total_epochs_across_phases, eta_min=lr * 0.01)
    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max=total_epochs_across_phases, eta_min=lr * 0.01)
    
    global_start_epoch = 0 #è¡¨ç¤ºå·²å®Œæˆçš„å…¨å±€epochæ•°
    current_phase_idx_resume = 0
    epoch_in_phase_start_resume = 0 #è¡¨ç¤ºå½“å‰é˜¶æ®µå·²å®Œæˆçš„epochæ•°
    phase_info_loaded = {}

    if resume_from_checkpoint:
        latest_json_path = 'checkpoints/latest.json'
        if os.path.exists(latest_json_path): # æ‰€æœ‰è¿›ç¨‹æ£€æŸ¥ï¼Œä½†åŠ è½½ç”±load_checkpointå¤„ç†
            with open(latest_json_path, 'r', encoding='utf-8') as f_json: # é‡å‘½å f
                latest_info = json.load(f_json)
            checkpoint_path_to_load = latest_info['latest_checkpoint']
            
            # æ‰€æœ‰è¿›ç¨‹åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
            # netG_module, netD_module æ˜¯æœªåŒ…è£…æˆ– .module çš„æ¨¡å‹
            global_start_epoch, phase_info_loaded = load_checkpoint(
                checkpoint_path_to_load, netG_module, netD_module, optimizerG, optimizerD, scaler, device, rank
            )
            
            if phase_info_loaded: 
                current_phase_idx_resume = phase_info_loaded.get('phase_idx', 0)
                # epoch_in_phase in phase_info is the epoch *just completed* or about to start if 0
                # If it's 'epoch_in_phase': 1, it means 1 epoch of that phase was done. So next is 1.
                epoch_in_phase_start_resume = phase_info_loaded.get('epoch_in_phase', 0) 

            # æ‰€æœ‰è¿›ç¨‹åŒæ­¥è°ƒåº¦å™¨çŠ¶æ€
            for _ in range(global_start_epoch): 
                schedulerG.step()
                schedulerD.step()
        elif is_main_process(rank):
            print(f"Rank {rank}: æœªæ‰¾åˆ° latest.jsonï¼Œä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    if ddp_active: # DDP Barrier: ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨å¼€å§‹è®­ç»ƒå‰éƒ½å·²å®ŒæˆåŠ è½½æˆ–åˆå§‹åŒ–
        dist.barrier()

    if is_main_process(rank): print(f"Rank {rank}: å¼€å§‹è®­ç»ƒ...")
    current_global_epoch = global_start_epoch # å½“å‰æ­£åœ¨è¿›è¡Œçš„å…¨å±€epoch (0-indexed)
    
    for phase_idx in range(current_phase_idx_resume, len(training_phases)):
        phase = training_phases[phase_idx]
        current_target_size = phase['size']
        phase_total_epochs = phase['epochs']
        
        current_dataloader = dataloaders.get(current_target_size)
        if current_dataloader is None:
            if is_main_process(rank):
                print(f"Rank {rank}: è·³è¿‡é˜¶æ®µ {phase_idx + 1} (å°ºå¯¸ {current_target_size}x{current_target_size}) å› ä¸ºæ•°æ®åŠ è½½å™¨ä¸ºç©ºã€‚")
            continue
        
        # DDP: set epoch for DistributedSampler, to ensure shuffling varies across epochs
        if ddp_active and hasattr(current_dataloader.sampler, 'set_epoch'):
            current_dataloader.sampler.set_epoch(current_global_epoch) # Use global epoch for consistent shuffling state if resuming

        if is_main_process(rank):
            print(f"\nRank {rank}: === é˜¶æ®µ {phase_idx + 1}/{len(training_phases)}: è®­ç»ƒ {current_target_size}x{current_target_size} å°ºå¯¸ ===")
        
        # å¦‚æœæ˜¯ä»è¿™ä¸ªé˜¶æ®µæ¢å¤ï¼Œstart_e_in_phase æ˜¯å·²å®Œæˆçš„epochæ•°ï¼Œæ‰€ä»¥ä»å®ƒå¼€å§‹
        start_e_in_phase = epoch_in_phase_start_resume if phase_idx == current_phase_idx_resume else 0
        
        for epoch_in_phase in range(start_e_in_phase, phase_total_epochs):
            epoch_start_time = datetime.now()
            netG.train() # Sets DDP-wrapped model to train mode
            netD.train()
            
            for i_batch, real_images in enumerate(current_dataloader):
                real_images = real_images.to(device)
                batch_actual_size = real_images.size(0) # Per-GPU batch size
                
                real_label_val = 0.9 
                fake_label_val = 0.1
                real_label = torch.full((batch_actual_size,), real_label_val, dtype=torch.float32, device=device)
                fake_label = torch.full((batch_actual_size,), fake_label_val, dtype=torch.float32, device=device)

                # --- è®­ç»ƒåˆ¤åˆ«å™¨ ---
                optimizerD.zero_grad(set_to_none=True)
                
                noise = torch.randn(batch_actual_size, nz, device=device)
                fake_images, output_real, loss_D_real, output_fake_detached, loss_D_fake, loss_D = None, None, None, None, None, None

                with autocast(device_type=device.type, enabled=amp_enabled, dtype=amp_dtype):
                    fake_images = netG(noise, target_size=current_target_size) # DDP forward

                with autocast(device_type=device.type, enabled=amp_enabled, dtype=amp_dtype):
                    output_real = netD(real_images) # DDP forward
                    loss_D_real = criterion(output_real, real_label)
                    
                    output_fake_detached = netD(fake_images.detach()) # DDP forward
                    loss_D_fake = criterion(output_fake_detached, fake_label)
                    loss_D = (loss_D_real + loss_D_fake) * 0.5
                
                if loss_D is not None:
                    scaler.scale(loss_D).backward() # DDP handles gradient sync
                    if grad_clip_norm > 0:
                        scaler.unscale_(optimizerD) 
                        torch.nn.utils.clip_grad_norm_(netD.parameters(), grad_clip_norm)
                    scaler.step(optimizerD)

                # --- è®­ç»ƒç”Ÿæˆå™¨ ---
                optimizerG.zero_grad(set_to_none=True)
                output_fake_for_G, loss_G = None, None
                if fake_images is not None: 
                    with autocast(device_type=device.type, enabled=amp_enabled, dtype=amp_dtype):
                        output_fake_for_G = netD(fake_images) # DDP forward (new graph for G)
                        loss_G = criterion(output_fake_for_G, real_label) 
                    
                    if loss_G is not None:
                        scaler.scale(loss_G).backward() # DDP handles gradient sync
                        if grad_clip_norm > 0:
                            scaler.unscale_(optimizerG)
                            torch.nn.utils.clip_grad_norm_(netG.parameters(), grad_clip_norm)
                        scaler.step(optimizerG)

                scaler.update()

                loss_D_item = loss_D.item() if loss_D is not None else float('nan')
                loss_G_item = loss_G.item() if loss_G is not None else float('nan')
                # For DDP, these means are per-process. Could aggregate if needed for global stats.
                output_real_mean = output_real.mean().item() if output_real is not None else float('nan')
                output_fake_detached_mean = output_fake_detached.mean().item() if output_fake_detached is not None else float('nan')
                output_fake_for_G_mean = output_fake_for_G.mean().item() if output_fake_for_G is not None else float('nan')

                if i_batch % 50 == 0 and is_main_process(rank): # Log only from main process
                    current_lr_G = optimizerG.param_groups[0]['lr']
                    current_lr_D = optimizerD.param_groups[0]['lr']
                    print(f'[R{rank}][Ph{phase_idx+1}][GEp {current_global_epoch+1}/{total_epochs_across_phases}] '
                          f'[PhEp {epoch_in_phase+1}/{phase_total_epochs}] [{i_batch}/{len(current_dataloader)}] '
                          f'L_D: {loss_D_item:.4f} L_G: {loss_G_item:.4f} '
                          f'D(x): {output_real_mean:.4f} '
                          f'D(G(z)): {output_fake_detached_mean:.4f}(D)/{output_fake_for_G_mean:.4f}(G) '
                          f'LR_G: {current_lr_G:.6f} LR_D: {current_lr_D:.6f} Scale: {scaler.get_scale():.1f}')
            
            # End of an epoch_in_phase
            current_global_epoch += 1 
            schedulerG.step() # Step schedulers for all processes
            schedulerD.step()

            # Save samples and checkpoint (only from main process)
            if is_main_process(rank):
                if (epoch_in_phase + 1) % 5 == 0 or (epoch_in_phase + 1) == phase_total_epochs:
                    save_sample_images(netG_module, current_global_epoch, current_target_size, nz, 
                                       device=device, amp_enabled=amp_enabled)
                    
                    current_phase_progress_info = {
                        'phase_idx': phase_idx,
                        'epoch_in_phase': epoch_in_phase + 1, # epoch_in_phase just completed
                        'size': current_target_size
                    }
                    save_checkpoint(netG_module, netD_module, optimizerG, optimizerD, scaler, 
                                    current_global_epoch, current_phase_progress_info, # current_global_epoch is epochs completed
                                    amp_enabled=amp_enabled, rank=rank)

                epoch_duration = datetime.now() - epoch_start_time
                print(f"Rank {rank}: é˜¶æ®µEpoch {epoch_in_phase+1} å®Œæˆï¼Œè€—æ—¶: {epoch_duration}")
            
            if ddp_active: # Barrier to ensure all processes complete epoch before next phase/sampler update
                dist.barrier()


        epoch_in_phase_start_resume = 0 # Reset for the next phase
        if is_main_process(rank):
            print(f"Rank {rank}: é˜¶æ®µ {phase_idx + 1} ({current_target_size}x{current_target_size}) è®­ç»ƒå®Œæˆã€‚")
    
    if is_main_process(rank):
        print("\n=== æ‰€æœ‰è®­ç»ƒé˜¶æ®µå®Œæˆï¼ ===")
        final_model_dir = 'final_models_stylegan3_inspired'
        os.makedirs(final_model_dir, exist_ok=True)
        torch.save(netG_module.state_dict(), os.path.join(final_model_dir, 'generator_final.pth'))
        torch.save(netD_module.state_dict(), os.path.join(final_model_dir, 'discriminator_final.pth'))
        
        config_summary = {
            'total_global_epochs_trained': current_global_epoch,
            'batch_size_per_gpu': batch_size,
            'world_size': world_size,
            'global_batch_size': batch_size * world_size,
            'initial_learning_rate': lr,
            'noise_dim': nz,
            'blur_kernel_size_for_antialiasing': blur_kernel_size,
            'training_phases_config': training_phases,
            'amp_enabled_during_training': amp_enabled,
            'amp_dtype_used': str(amp_dtype) if amp_enabled else 'N/A',
            'ddp_active': ddp_active
        }
        with open(os.path.join(final_model_dir, 'training_config.json'), 'w', encoding='utf-8') as f_json_out: # Renamed f
            json.dump(config_summary, f_json_out, ensure_ascii=False, indent=2)
        print(f"Rank {rank}: æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_dir}")
    
    return netG_module, netD_module

# --- ç”Ÿæˆå›¾ç‰‡ (é€šå¸¸åœ¨å•è¿›ç¨‹/ä¸»è¿›ç¨‹æ‰§è¡Œ) ---
def generate_images(model_path, num_images=10, sizes_to_generate=[64, 128, 256, 512], 
                    nz=100, blur_kernel_size=3, rank=0): # rank for logging
    if not is_main_process(rank): # Generation only on main process
        return

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # Use default GPU for generation
    amp_enabled_inference = (device.type == 'cuda') 
    amp_dtype_inference = torch.float16 if amp_enabled_inference else torch.float32

    print(f"Rank {rank}: ä½¿ç”¨è®¾å¤‡è¿›è¡Œç”Ÿæˆ: {device}, AMPæ¨ç†: {'å¯ç”¨' if amp_enabled_inference else 'æœªå¯ç”¨'}, Dtype: {amp_dtype_inference if amp_enabled_inference else 'default'}")
    try:
        # Initialize base model, not DDP wrapped
        netG = Generator(nz=nz, ngf=64, nc=3, blur_kernel_size=blur_kernel_size).to(device)
        if not os.path.exists(model_path):
            print(f"Rank {rank}: é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}")
            return
        
        # state_dict should be from netG.module if saved from DDP training
        state_dict = torch.load(model_path, map_location=device)
        netG.load_state_dict(state_dict)
        netG.eval()
        print(f"Rank {rank}: æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")

        with torch.no_grad():
            for size_val_gen in sizes_to_generate:
                print(f"\nRank {rank}: æ­£åœ¨ç”Ÿæˆ {size_val_gen}x{size_val_gen} å°ºå¯¸çš„å›¾ç‰‡...")
                save_dir = f'final_generated_stylegan3_inspired_{size_val_gen}x{size_val_gen}'
                os.makedirs(save_dir, exist_ok=True)
                
                for i_gen in range(num_images):
                    noise = torch.randn(1, nz, device=device)
                    fake_image = None
                    with autocast(device_type=device.type, enabled=amp_enabled_inference, dtype=amp_dtype_inference):
                        fake_image = netG(noise, target_size=size_val_gen)
                    
                    if fake_image is not None:
                        fake_image = (fake_image + 1) / 2.0
                        fake_image = torch.clamp(fake_image, 0, 1)
                        
                        img_tensor_chw = fake_image[0]
                        img_np_hwc = img_tensor_chw.cpu().float().numpy().transpose(1, 2, 0) 
                        img_pil = Image.fromarray((img_np_hwc * 255).astype(np.uint8))
                        img_path = os.path.join(save_dir, f'generated_{i_gen+1:03d}.png')
                        img_pil.save(img_path)
                        if (i_gen + 1) % 5 == 0 or (i_gen + 1) == num_images:
                            print(f"  Rank {rank}: å·²ç”Ÿæˆ {i_gen+1}/{num_images} å¼ å›¾ç‰‡ (å°ºå¯¸ {size_val_gen}x{size_val_gen})")
                print(f"Rank {rank}: âœ“ {size_val_gen}x{size_val_gen} å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼Œä¿å­˜åœ¨ {save_dir} ç›®å½•")
        print(f"\nRank {rank}: æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼")
    except Exception as e:
        print(f"Rank {rank}: ç”Ÿæˆå›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

# --- äº¤äº’æ¨¡å¼å’Œä¸»å‡½æ•° ---

def create_dummy_data_if_not_exists(data_dir, num_dummy_images=16, rank=0):
    if not is_main_process(rank): # Only main process creates data
        return

    if not os.path.exists(data_dir):
        print(f"Rank {rank}: æ•°æ®ç›®å½• {data_dir} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºå®ƒã€‚")
        os.makedirs(data_dir, exist_ok=True)
    
    img_extensions = ('.png', '.jpg', '.jpeg')
    try:
        image_files = [f for f in os.listdir(data_dir) if f.lower().endswith(img_extensions)]
        if not image_files:
            print(f"Rank {rank}: æ•°æ®ç›®å½• {data_dir} ä¸ºç©ºï¼Œå°†åˆ›å»º {num_dummy_images} å¼ è™šæ‹ŸPNGå›¾ç‰‡ã€‚")
            for i_dummy in range(num_dummy_images):
                color = (np.random.randint(0,255), np.random.randint(0,128), np.random.randint(128,255))
                dummy_img = Image.new('RGB', (512,512), color=color)
                dummy_img.save(os.path.join(data_dir, f"dummy_image_{i_dummy+1}.png"))
            print(f"Rank {rank}: å·²åˆ›å»º {num_dummy_images} å¼ è™šæ‹Ÿå›¾ç‰‡åœ¨ {data_dir}ã€‚")
    except OSError as e:
        print(f"Rank {rank}: æ£€æŸ¥æˆ–åˆ›å»ºè™šæ‹Ÿæ•°æ®æ—¶å‘ç”ŸOSé”™è¯¯ (å¯èƒ½æ˜¯æƒé™é—®é¢˜): {e} in {data_dir}")
    except Exception as e:
        print(f"Rank {rank}: åˆ›å»ºè™šæ‹Ÿå›¾ç‰‡å¤±è´¥: {e}")


def configure_training_params(resume=False): # Assumed to be run by main process only if interactive
    print(f"\n{'â¯ï¸  é…ç½®æ¢å¤è®­ç»ƒå‚æ•°' if resume else 'ğŸš€ é…ç½®æ–°è®­ç»ƒå‚æ•°'}")
    
    default_data_dir = "./product_images"
    # create_dummy_data_if_not_exists is called by main process later if needed

    data_dir_input_val = input(f"æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: {default_data_dir}): ").strip()
    data_dir = data_dir_input_val or default_data_dir
    
    # This validation loop should ideally be before DDP setup if interactive
    img_extensions = ('.png', '.jpg', '.jpeg')
    while True:
        # This function is called only by rank 0 if interactive.
        # So, create_dummy_data_if_not_exists will also be called by rank 0.
        create_dummy_data_if_not_exists(data_dir, rank=0) # Pass rank explicitly

        if os.path.exists(data_dir) and os.path.isdir(data_dir):
            try:
                if any(f.lower().endswith(img_extensions) for f in os.listdir(data_dir)):
                    break 
                else:
                    print(f"é”™è¯¯: ç›®å½• '{data_dir}' ä¸åŒ…å«PNG/JPG/JPEGå›¾ç‰‡ã€‚å·²å°è¯•åˆ›å»ºè™šæ‹Ÿæ•°æ®ã€‚")
            except OSError as e:
                print(f"é”™è¯¯: æ— æ³•è®¿é—®ç›®å½• '{data_dir}' ä¸­çš„æ–‡ä»¶ (æƒé™é—®é¢˜?): {e}")
        else:
            print(f"é”™è¯¯: ç›®å½• '{data_dir}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•ã€‚å·²å°è¯•åˆ›å»ºé»˜è®¤ç›®å½•å’Œè™šæ‹Ÿæ•°æ®ã€‚")

        data_dir_input_val = input(f"è¯·é‡æ–°è¾“å…¥æœ‰æ•ˆçš„æ•°æ®ç›®å½•è·¯å¾„ (æˆ–æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤ '{default_data_dir}'): ").strip()
        data_dir = data_dir_input_val or default_data_dir


    batch_size_str = input("æ¯GPUæ‰¹æ¬¡å¤§å° (é»˜è®¤: 2, å¦‚æœGPUå†…å­˜å…è®¸å¯å°è¯•4-8): ").strip() or "2"
    batch_size = int(batch_size_str)
    learning_rate = float(input("å­¦ä¹ ç‡ (é»˜è®¤: 0.0002): ").strip() or 0.0002)
    noise_dim = int(input("å™ªå£°å‘é‡ç»´åº¦ (é»˜è®¤: 100): ").strip() or 100)
    blur_k_size = int(input("æŠ—æ··å æ¨¡ç³Šæ ¸å¤§å° (0ç¦ç”¨, 3æˆ–5æ¨è, é»˜è®¤3): ").strip() or 3)
    num_epochs_per_phase_str = input("æ¯ä¸ªé˜¶æ®µçš„Epochæ•° (å¯é€‰, é»˜è®¤æŒ‰ä»£ç å†…è®¾ç½®): ").strip()
    num_epochs_per_phase = int(num_epochs_per_phase_str) if num_epochs_per_phase_str else None
    num_workers_str = input("DataLoaderå·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 0 for win, 2 for linux, CPUå…è®¸å¯è®¾æ›´é«˜): ").strip()
    default_workers = 2 if os.name != 'nt' else 0 # Better default for Linux
    num_workers = int(num_workers_str) if num_workers_str else default_workers
    grad_clip_str = input(f"æ¢¯åº¦è£å‰ªèŒƒæ•° (é»˜è®¤1.0, è¾“å…¥0ç¦ç”¨): ").strip() or "1.0"
    grad_clip_norm = float(grad_clip_str)

    return {
        'mode': 'train',
        'data_dir': data_dir,
        'batch_size': batch_size,
        'learning_rate': learning_rate,
        'noise_dim': noise_dim,
        'resume': resume,
        'blur_kernel_size': blur_k_size,
        'num_epochs_per_phase': num_epochs_per_phase,
        'num_workers': num_workers,
        'grad_clip_norm': grad_clip_norm
    }

def configure_generation_params(): # Assumed to be run by main process only
    print("\nğŸ¨ é…ç½®å›¾ç‰‡ç”Ÿæˆå‚æ•°")
    default_model = "final_models_stylegan3_inspired/generator_final.pth"
    model_path = input(f"æ¨¡å‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_model}): ").strip() or default_model
    while not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ '{model_path}' ä¸å­˜åœ¨ã€‚")
        model_path = input(f"è¯·é‡æ–°è¾“å…¥æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_model}): ").strip() or default_model

    num_generate = int(input("ç”Ÿæˆå›¾ç‰‡æ•°é‡ (é»˜è®¤: 4): ").strip() or 4)
    noise_dim = int(input("å™ªå£°å‘é‡ç»´åº¦ (é»˜è®¤: 100, éœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´): ").strip() or 100)
    blur_k_size = int(input("æŠ—æ··å æ¨¡ç³Šæ ¸å¤§å° (ç”¨äºæ¨¡å‹åˆå§‹åŒ–, éœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´, é»˜è®¤3): ").strip() or 3)
    
    print("\né€‰æ‹©è¦ç”Ÿæˆçš„å›¾ç‰‡å°ºå¯¸ (å¯å¤šé€‰ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ 1,3):")
    print("1. 64x64")
    print("2. 128x128")
    print("3. 256x256")
    print("4. 512x512")
    print("5. æ‰€æœ‰å°ºå¯¸ (64, 128, 256, 512)")
    
    size_map_single = {'1': 64, '2': 128, '3': 256, '4': 512}
    all_sizes_list = [64, 128, 256, 512]
    sizes_to_generate_list = []
    
    while True:
        choices_str = input("è¯·é€‰æ‹© (é»˜è®¤: 5): ").strip() or '5'
        if choices_str == '5':
            sizes_to_generate_list = all_sizes_list
            break
        try:
            selected_indices = [s.strip() for s in choices_str.split(',')]
            sizes_to_generate_list = []
            valid_choices = True
            for idx_str in selected_indices:
                if idx_str in size_map_single:
                    sizes_to_generate_list.append(size_map_single[idx_str])
                else:
                    valid_choices = False
                    break
            if valid_choices and sizes_to_generate_list:
                sizes_to_generate_list = sorted(list(set(sizes_to_generate_list)))
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©æˆ–æœªé€‰æ‹©ä»»ä½•å°ºå¯¸ã€‚")
        except Exception as e:
            print(f"âŒ è¾“å…¥æ ¼å¼é”™è¯¯: {e}")

    return {
        'mode': 'generate',
        'model_path': model_path,
        'num_generate': num_generate,
        'noise_dim': noise_dim,
        'sizes_to_generate': sizes_to_generate_list,
        'blur_kernel_size': blur_k_size
    }

def interactive_mode(): # Assumed to be run by main process only
    print("\n" + "=" * 60)
    print("ğŸ¯ PyTorch GAN (StyleGAN3å¯å‘å¼ä¼˜åŒ– & DDP) - äº¤äº’æ¨¡å¼")
    print("=" * 60)
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. ğŸš€ å¼€å§‹æ–°çš„è®­ç»ƒ")
    print("2. â¯ï¸  ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    print("3. ğŸ¨ ä»…ç”Ÿæˆå›¾ç‰‡ï¼ˆéœ€è¦å·²è®­ç»ƒçš„æ¨¡å‹ï¼‰")
    print("0. ğŸšª é€€å‡ºç¨‹åº")
    
    config_choice = None
    while True:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-3): ").strip()
        if choice == '0': 
            config_choice = None
            break
        if choice == '1': 
            config_choice = configure_training_params(resume=False)
            break
        if choice == '2': 
            config_choice = configure_training_params(resume=True)
            break
        if choice == '3': 
            config_choice = configure_generation_params()
            break
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥0-3ä¹‹é—´çš„æ•°å­—")
    return config_choice


def main():
    # DDP setup: RANK and WORLD_SIZE are set by torchrun
    # LOCAL_RANK is also set by torchrun for node-local rank
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    # For single node, local_rank is often the same as rank.
    # If using torch.cuda.set_device, local_rank is the key.
    # torchrun sets LOCAL_RANK, if not, assume it's rank for single node.
    local_rank = int(os.environ.get("LOCAL_RANK", rank)) 

    ddp_active = world_size > 1
    if ddp_active:
        setup_ddp(rank, world_size) # rank here is global rank
    
    set_seed(42, rank) # Set seed for all processes

    config = None
    if is_main_process(rank): # Interactive mode only for main process
        if not ddp_active: # If not DDP, run interactive mode fully
            print("\nğŸ¯ å¯åŠ¨äº¤äº’æ¨¡å¼ (å•è¿›ç¨‹)...")
            config = interactive_mode()
        else: # If DDP, main process can still take input, but needs to broadcast
              # For simplicity here, if DDP, we skip interactive and use defaults
              # This part would need a proper CLI arg parser for DDP
            print(f"Rank {rank}: DDPæ¨¡å¼æ¿€æ´» (world_size={world_size}). "
                  "è·³è¿‡äº¤äº’å¼é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®­ç»ƒå‚æ•°ã€‚")
            default_data_dir = "./product_images"
            create_dummy_data_if_not_exists(default_data_dir, rank=rank) # Main process creates
            
            config = { # Example default config for DDP training
                'mode': 'train', 'data_dir': default_data_dir, 'batch_size': 2,
                'learning_rate': 0.0002, 'noise_dim': 100, 'resume': False,
                'blur_kernel_size': 3, 'num_epochs_per_phase': 2, # Shorter for demo
                'num_workers': 2 if os.name != 'nt' else 0, 'grad_clip_norm': 1.0
            }
    
    if ddp_active:
        # Broadcast config from rank 0 to all other processes
        # This requires config to be picklable.
        config_list = [config] # Put config in a list to use broadcast_object_list
        dist.broadcast_object_list(config_list, src=0)
        if not is_main_process(rank): # Other processes get config from broadcast
            config = config_list[0]

    if config is None: # If main process chose to exit, or broadcast failed (though simple dict should be fine)
        if is_main_process(rank): print("ğŸ‘‹ ç¨‹åºé€€å‡ºã€‚")
        if ddp_active: cleanup_ddp()
        return

    if is_main_process(rank):
        print("=" * 60)
        print(f"ğŸ¯ PyTorch GAN - ä»»åŠ¡æ‰§è¡Œ (Rank {rank}/{world_size})")
        print("=" * 60)

    try:
        if config['mode'] == 'generate':
            if is_main_process(rank): # Generation only on main process
                print("\nğŸ¨ ç”Ÿæˆé…ç½®:")
                for k_conf, v_conf in config.items(): print(f"   {k_conf}: {v_conf}")
                print("-" * 30)
            # Pass rank for logging, generate_images itself has a main_process guard
            generate_images(
                model_path=config['model_path'],
                num_images=config['num_generate'],
                sizes_to_generate=config['sizes_to_generate'],
                nz=config['noise_dim'],
                blur_kernel_size=config['blur_kernel_size'],
                rank=rank 
            )
        elif config['mode'] == 'train':
            if is_main_process(rank):
                print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
                for k_conf, v_conf in config.items(): print(f"   {k_conf}: {v_conf}")
                print("-" * 30)
            
            # Barrier before training to ensure all configs are set and data (dummy) might be created
            if ddp_active:
                dist.barrier()

            netG_final, netD_final = train_gan( 
                data_dir=config['data_dir'],
                num_epochs_override=config.get('num_epochs_per_phase'),
                batch_size=config['batch_size'],
                lr=config['learning_rate'],
                nz=config['noise_dim'],
                resume_from_checkpoint=config['resume'],
                blur_kernel_size=config['blur_kernel_size'],
                num_workers=config['num_workers'],
                grad_clip_norm=config['grad_clip_norm'],
                rank=rank, world_size=world_size, ddp_active=ddp_active
            )
            
            if is_main_process(rank): print("\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆï¼")
            
            if is_main_process(rank): # Final sample generation by main process
                final_model_path = f'final_models_stylegan3_inspired/generator_final.pth'
                if os.path.exists(final_model_path):
                    print("\nğŸ¨ å°è¯•ä½¿ç”¨æœ€ç»ˆè®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆä¸€äº›æµ‹è¯•å›¾ç‰‡...")
                    generate_images(
                        model_path=final_model_path, 
                        num_images=4, 
                        sizes_to_generate=[64, 128, 256, 512], 
                        nz=config['noise_dim'], 
                        blur_kernel_size=config['blur_kernel_size'],
                        rank=rank
                    )
                else:
                    print(f"æœªæ‰¾åˆ°æœ€ç»ˆæ¨¡å‹ {final_model_path}ï¼Œè·³è¿‡æœ€ç»ˆæ ·æœ¬ç”Ÿæˆã€‚")

    except KeyboardInterrupt:
        if is_main_process(rank): print("\n\nâ¸ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        if is_main_process(rank): print(f"\nâŒ å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if ddp_active:
            cleanup_ddp()
        if is_main_process(rank): print("\nğŸ‘‹ ç¨‹åºæ‰§è¡Œå®Œæ¯•ã€‚")

if __name__ == "__main__":
    main()
