# -*- coding: utf-8 -*-
"""
å¤šå¡è®­ç»ƒ
GANè®­ç»ƒå™¨ - æ”¯æŒemojiç¬¦å·å’ŒUnicodeå­—ç¬¦ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
ä½¿ç”¨UTF-8ç¼–ç ç¡®ä¿æ‰€æœ‰å­—ç¬¦æ­£ç¡®æ˜¾ç¤ºå’Œä¿å­˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
import torch.multiprocessing as mp
from torchvision.utils import save_image
import torchvision.utils as vutils
from torch.utils.data import random_split
from torchvision import transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import animation
from torch.amp import GradScaler, autocast  # ä½¿ç”¨æ–°çš„API
from IPython.display import HTML
import os
import time
import logging
import logging.config
import argparse

def setup_utf8_environment():
    """è®¾ç½®UTF-8ç¯å¢ƒï¼Œç¡®ä¿emojiç¬¦å·èƒ½æ­£ç¡®æ˜¾ç¤º"""
    import sys
    
    # ä¸ºWindowsè®¾ç½®UTF-8ç¯å¢ƒ
    if os.name == 'nt':  # Windowsç³»ç»Ÿ
        try:
            # è®¾ç½®æ§åˆ¶å°ä»£ç é¡µä¸ºUTF-8
            os.system('chcp 65001 > nul')
            
            # é‡æ–°é…ç½®æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
            if hasattr(sys.stdout, 'reconfigure'):
                sys.stdout.reconfigure(encoding='utf-8', errors='replace')
            if hasattr(sys.stderr, 'reconfigure'):
                sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•è®¾ç½®UTF-8ç¯å¢ƒ: {e}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTHONIOENCODING'] = 'utf-8'

def safe_emoji(emoji_text, fallback_text):
    """å®‰å…¨çš„emojiæ˜¾ç¤ºå‡½æ•°ï¼Œåœ¨ä¸æ”¯æŒçš„ç¯å¢ƒä¸­ä½¿ç”¨æ›¿ä»£æ–‡æœ¬"""
    try:
        # æµ‹è¯•æ˜¯å¦èƒ½ç¼–ç emoji
        emoji_text.encode('utf-8')
        return emoji_text
    except UnicodeEncodeError:
        return fallback_text

def setup_distributed(rank, world_size, backend='nccl'):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(backend, rank=rank, world_size=world_size)
    
    # è®¾ç½®å½“å‰è¿›ç¨‹çš„GPU
    if torch.cuda.is_available():
        torch.cuda.set_device(rank)

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    dist.destroy_process_group()

def get_world_size():
    """è·å–æ€»è¿›ç¨‹æ•°"""
    if dist.is_available() and dist.is_initialized():
        return dist.get_world_size()
    return 1

def get_rank():
    """è·å–å½“å‰è¿›ç¨‹çš„rank"""
    if dist.is_available() and dist.is_initialized():
        return dist.get_rank()
    return 0

def is_main_process():
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return get_rank() == 0

# åœ¨æ¨¡å—åŠ è½½æ—¶ç«‹å³è®¾ç½®UTF-8ç¯å¢ƒ
setup_utf8_environment()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def collate_fn(batch):
    """æ‰¹é‡æ•°æ®æ•´ç†å‡½æ•°ï¼Œè¿”å›CPU tensorä»¥æ”¯æŒpin_memoryï¼Œåœ¨è®­ç»ƒå¾ªç¯ä¸­å†ç§»åŠ¨åˆ°GPU"""
    images, labels = zip(*batch)
    
    # è¿”å›CPU tensorï¼Œè®©DataLoaderçš„pin_memoryå¤„ç†GPUä¼ è¾“ä¼˜åŒ–
    images = torch.stack(images)
    labels = torch.tensor(labels)
    return images, labels

class MyDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.png'))]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.image_files[idx])
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        # è¿”å›tensorç±»å‹çš„æ ‡ç­¾ä»¥ä¿æŒä¸€è‡´æ€§
        return image, torch.tensor(0)

class Generator(nn.Module):
    def __init__(self, latent_dim, num_layers, base_channels):
        super(Generator, self).__init__()
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.ConvTranspose2d(latent_dim, base_channels * 8, 4, 1, 0, bias=False))
            else:
                layers.append(nn.ConvTranspose2d(base_channels * (8 // (2 ** (i - 1))), base_channels * (8 // (2 ** i)), 4, 2, 1, bias=False))
            layers.append(nn.BatchNorm2d(base_channels * (8 // (2 ** i))))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
        layers.append(nn.ConvTranspose2d(base_channels, 3, 4, 2, 1, bias=False))
        layers.append(nn.Tanh())
        self.main = nn.Sequential(*layers)


    def forward(self, input):
        return self.main(input.to(torch.float32))# å°†æƒé‡ç±»å‹è®¾ç½®ä¸ºå•ç²¾åº¦æµ®ç‚¹æ•°

class Discriminator(nn.Module):
    def __init__(self, num_layers, base_channels):
        super(Discriminator, self).__init__()
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.Conv2d(3, base_channels, 4, 2, 1, bias=False))
            else:
                layers.append(nn.Conv2d(base_channels * (2 ** (i - 1)), base_channels * (2 ** i), 4, 2, 1, bias=False))
                layers.append(nn.BatchNorm2d(base_channels * (2 ** i)))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
        layers.append(nn.Conv2d(base_channels * (2 ** (num_layers - 1)), 1, 4, 1, 0, bias=False))
        self.main = nn.Sequential(*layers)

    def forward(self, input):
        return self.main(input.to(torch.float32))

class GANTrainer:
    def __init__(self, dataset_path, latent_dim=100, lr_G=0.0002, lr_D=0.0002, betas=(0.5, 0.999), batch_size=128, image_size=64,
                 epochs=50,start_epoch=0,patience=5, min_delta=0.0001, num_layers=4, base_channels=64, load_models=False,gradient_accumulation_steps=1, validation_frequency=10, 
                 distributed=False, rank=0, world_size=1):
        self.dataset_path = dataset_path
        self.latent_dim = latent_dim
        self.lr_G = lr_G
        self.lr_D = lr_D
        self.betas = betas
        self.batch_size = batch_size
        self.image_size = image_size
        self.epochs = epochs
        self.start_epoch=start_epoch
        self.patience = patience
        self.min_delta = min_delta
        self.num_layers = num_layers
        self.base_channels = base_channels
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.validation_frequency = validation_frequency  # æ–°å¢éªŒè¯é¢‘ç‡å‚æ•°ï¼Œä¸patienceè¯­ä¹‰åˆ†ç¦»
        self.load_models = load_models  # ä¿å­˜åŠ è½½æ¨¡å‹æ ‡å¿—
        self.distributed = distributed  # åˆ†å¸ƒå¼è®­ç»ƒæ ‡å¿—
        self.rank = rank  # å½“å‰è¿›ç¨‹rank
        self.world_size = world_size  # æ€»è¿›ç¨‹æ•°
        self.best_val_loss = float('inf')  # è®¾ç½®ä¸€ä¸ªåˆå§‹å€¼ï¼Œä¾‹å¦‚æ­£æ— ç©·å¤§

        # è®¾ç½®è®¾å¤‡ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
        if self.distributed and torch.cuda.is_available():
            self.device = torch.device(f'cuda:{self.rank}')
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç»Ÿä¸€Loggerç®¡ç† - åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºlogger
        if is_main_process():
            self.logger = self._setup_logger()
            self.logger.debug(f'å½“å‰ä½¿ç”¨çš„è®¾å¤‡æ˜¯ï¼š{self.device}')
            if self.distributed:
                self.logger.info(f'{safe_emoji("ğŸŒ", "[DISTRIBUTED]")} åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼ï¼šRank {self.rank}/{self.world_size-1}')
        else:
            # éä¸»è¿›ç¨‹ä½¿ç”¨ç®€å•çš„loggeræˆ–è€…None
            self.logger = self._setup_simple_logger()

        self.generator = Generator(latent_dim, num_layers, base_channels).to(self.device)
        self.discriminator = Discriminator(num_layers, base_channels).to(self.device)

        # åˆ†å¸ƒå¼è®­ç»ƒï¼šåŒ…è£…æ¨¡å‹
        if self.distributed:
            self.generator = DDP(self.generator, device_ids=[self.rank])
            self.discriminator = DDP(self.discriminator, device_ids=[self.rank])

        self.criterion = nn.BCEWithLogitsLoss()
        self.optimizer_G = torch.optim.AdamW(self.generator.parameters(), lr=self.lr_G, betas=betas)
        self.optimizer_D = torch.optim.AdamW(self.discriminator.parameters(), lr=self.lr_D, betas=betas)

        self.scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_G, T_max=self.epochs)
        self.scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_D, T_max=self.epochs)

        self.scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')  # ä½¿ç”¨æ–°çš„APIæ ¼å¼

        self.dataset = self.load_dataset()
        self.train_dataloader, self.val_dataloader = self.split_dataset()
        
        plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

        self.train_epoch_dir = 'train_epoch'  # Specify the directory for saving files

        # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•
        if is_main_process() and not os.path.exists(self.train_epoch_dir):
            os.makedirs(self.train_epoch_dir)

    def _setup_logger(self):
        """ç»Ÿä¸€è®¾ç½®Loggeré…ç½®"""
        module_name = 'YPS'
        logger = logging.getLogger(f'{__name__}')
        
        # å¦‚æœloggerå·²ç»æœ‰handlerï¼Œå…ˆæ¸…ç©ºé¿å…é‡å¤
        if logger.handlers:
            logger.handlers.clear()
            
        logger.setLevel(logging.DEBUG)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists('train_epoch'):
            os.makedirs('train_epoch')

        # ä½¿ç”¨UTF-8ç¼–ç æ¥æ”¯æŒemojiç¬¦å·
        fh = logging.FileHandler(
            os.path.join('train_epoch', f"{module_name}_è®­ç»ƒlog.txt"), 
            encoding='utf-8'
        )
        fh.setLevel(logging.DEBUG)

        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        logger.addHandler(fh)
        logger.addHandler(ch)
        
        return logger

    def _setup_simple_logger(self):
        """ä¸ºéä¸»è¿›ç¨‹è®¾ç½®ç®€å•çš„logger"""
        logger = logging.getLogger(f'{__name__}_{self.rank}')
        logger.setLevel(logging.ERROR)  # éä¸»è¿›ç¨‹åªè®°å½•é”™è¯¯
        return logger

    def _load_models_unified(self):
        """ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½æ–¹æ³•ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ"""
        if not self.load_models:
            if is_main_process():
                self.logger.debug("æœªå¯ç”¨æ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            return 0
            
        # åªåœ¨ä¸»è¿›ç¨‹è¿›è¡Œæ—¥å¿—è¾“å‡º
        def log_info(msg):
            if is_main_process():
                self.logger.info(msg)
                
        def log_warning(msg):
            if is_main_process():
                self.logger.warning(msg)
                
        def log_debug(msg):
            if is_main_process():
                self.logger.debug(msg)
            
        # ä¼˜å…ˆçº§1: å°è¯•åŠ è½½å®Œæ•´çš„checkpoint
        checkpoint_path = os.path.join(self.train_epoch_dir, f'checkpoint_epoch_{self.start_epoch}to{self.epochs}.pth')
        if os.path.isfile(checkpoint_path):
            try:
                # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œmap_locationç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸ŠåŠ è½½
                map_location = {'cuda:0': f'cuda:{self.rank}'} if self.distributed else None
                checkpoint = torch.load(checkpoint_path, map_location=map_location)
                
                # åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œå¤„ç†DDPåŒ…è£…
                if self.distributed:
                    self.generator.module.load_state_dict(checkpoint['generator_state_dict'])
                    self.discriminator.module.load_state_dict(checkpoint['discriminator_state_dict'])
                else:
                    self.generator.load_state_dict(checkpoint['generator_state_dict'])
                    self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                    
                self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
                self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
                self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
                self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
                
                recovered_epoch = checkpoint['epoch']
                log_info(f"{safe_emoji('âœ…', '[SUCCESS]')} æˆåŠŸä»checkpointæ¢å¤è®­ç»ƒçŠ¶æ€ï¼Œä¸Šæ¬¡è®­ç»ƒåˆ°ç¬¬{recovered_epoch}ä¸ªepoch")
                log_debug("ç”Ÿæˆå™¨ç»“æ„ï¼š\n%s" % self.generator)
                log_debug("åˆ¤åˆ«å™¨ç»“æ„ï¼š\n%s" % self.discriminator)
                log_debug("ç”Ÿæˆå™¨å‚æ•°æ•°é‡ï¼š %s" % sum(p.numel() for p in self.generator.parameters()))
                log_debug("åˆ¤åˆ«å™¨å‚æ•°æ•°é‡ï¼š %s" % sum(p.numel() for p in self.discriminator.parameters()))
                
                return recovered_epoch + 1  # è¿”å›ä¸‹ä¸€ä¸ªè¦è®­ç»ƒçš„epoch
                
            except Exception as e:
                log_warning(f"{safe_emoji('âš ï¸', '[WARNING]')} CheckpointåŠ è½½å¤±è´¥ï¼š{e}")
        
        # ä¼˜å…ˆçº§2: å°è¯•åŠ è½½ç®€å•çš„æ¨¡å‹æ–‡ä»¶
        generator_path = f'generator_simple_epoch_{self.start_epoch}to{self.epochs}.pth'
        discriminator_path = f'discriminator_simple_epoch_{self.start_epoch}to{self.epochs}.pth'
        
        if os.path.isfile(generator_path) and os.path.isfile(discriminator_path):
            try:
                map_location = {'cuda:0': f'cuda:{self.rank}'} if self.distributed else None
                
                if self.distributed:
                    self.generator.module.load_state_dict(torch.load(generator_path, map_location=map_location))
                    self.discriminator.module.load_state_dict(torch.load(discriminator_path, map_location=map_location))
                else:
                    self.generator.load_state_dict(torch.load(generator_path, map_location=map_location))
                    self.discriminator.load_state_dict(torch.load(discriminator_path, map_location=map_location))
                    
                log_info(f"{safe_emoji('âœ…', '[SUCCESS]')} æˆåŠŸåŠ è½½ç®€å•æ¨¡å‹æ–‡ä»¶ï¼Œä»ç¬¬{self.start_epoch}ä¸ªepochå¼€å§‹è®­ç»ƒ")
                log_debug("ç”Ÿæˆå™¨ç»“æ„ï¼š\n%s" % self.generator)
                log_debug("åˆ¤åˆ«å™¨ç»“æ„ï¼š\n%s" % self.discriminator)
                return self.start_epoch
                
            except Exception as e:
                log_warning(f"{safe_emoji('âš ï¸', '[WARNING]')} ç®€å•æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        
        # ä¼˜å…ˆçº§3: å¯»æ‰¾å…¶ä»–å¯ç”¨çš„checkpointæ–‡ä»¶
        checkpoint_pattern = os.path.join(self.train_epoch_dir, 'checkpoint_epoch_*.pth')
        import glob
        checkpoint_files = glob.glob(checkpoint_pattern)
        if checkpoint_files:
            # é€‰æ‹©æœ€æ–°çš„checkpointæ–‡ä»¶
            latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
            try:
                map_location = {'cuda:0': f'cuda:{self.rank}'} if self.distributed else None
                checkpoint = torch.load(latest_checkpoint, map_location=map_location)
                
                if self.distributed:
                    self.generator.module.load_state_dict(checkpoint['generator_state_dict'])
                    self.discriminator.module.load_state_dict(checkpoint['discriminator_state_dict'])
                else:
                    self.generator.load_state_dict(checkpoint['generator_state_dict'])
                    self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                    
                self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
                self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
                self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
                self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
                
                recovered_epoch = checkpoint['epoch']
                log_info(f"{safe_emoji('âœ…', '[SUCCESS]')} æ‰¾åˆ°å¹¶åŠ è½½äº†å…¶ä»–checkpointæ–‡ä»¶ï¼š{os.path.basename(latest_checkpoint)}")
                log_info(f"æ¢å¤åˆ°ç¬¬{recovered_epoch}ä¸ªepoch")
                return recovered_epoch + 1
                
            except Exception as e:
                log_warning(f"{safe_emoji('âš ï¸', '[WARNING]')} å…¶ä»–checkpointåŠ è½½å¤±è´¥ï¼š{e}")
        
        # æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥
        log_info(f"{safe_emoji('â„¹ï¸', '[INFO]')} æœªæ‰¾åˆ°å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
        return 0

    #ä¿å­˜ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ‰€æœ‰æ•°æ®çš„æ¨¡å‹ - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
    def save_state(self, epoch):
        """ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼Œåªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ"""
        if not is_main_process():
            return
            
        checkpoint_path = os.path.join(self.train_epoch_dir, f'checkpoint_epoch_{self.start_epoch}to{self.epochs}.pth')
        
        # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œå¤„ç†DDPåŒ…è£…
        if self.distributed:
            generator_state_dict = self.generator.module.state_dict()
            discriminator_state_dict = self.discriminator.module.state_dict()
        else:
            generator_state_dict = self.generator.state_dict()
            discriminator_state_dict = self.discriminator.state_dict()
            
        torch.save({
            'epoch': epoch,
            'generator_state_dict': generator_state_dict,
            'discriminator_state_dict': discriminator_state_dict,
            'optimizer_G_state_dict': self.optimizer_G.state_dict(),
            'optimizer_D_state_dict': self.optimizer_D.state_dict(),
            'scheduler_G_state_dict': self.scheduler_G.state_dict(),
            'scheduler_D_state_dict': self.scheduler_D.state_dict(),
        }, checkpoint_path)

    def load_state(self):
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨å·²ç»è¢«_load_models_unifiedæ›¿ä»£ï¼Œä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§
        checkpoint_path = os.path.join(self.train_epoch_dir, f'checkpoint_epoch_{self.start_epoch}to{self.epochs}.pth')
        if os.path.isfile(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            self.generator.load_state_dict(checkpoint['generator_state_dict'])
            self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
            self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
            self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
            self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
            self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
            return checkpoint['epoch']
        else:
            return None

    def load_dataset(self):
        transform = transforms.Compose([
            transforms.Resize(self.image_size),
            transforms.CenterCrop(self.image_size),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        dataset = MyDataset(root_dir=self.dataset_path, transform=transform)
        return dataset

    def split_dataset(self):
        """æ‹†åˆ†æ•°æ®é›†ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ"""
        train_size = int(0.8 * len(self.dataset))
        val_size = len(self.dataset) - train_size
        train_dataset, val_dataset = random_split(self.dataset, [train_size, val_size])
        
        # åˆ†å¸ƒå¼è®­ç»ƒï¼šä½¿ç”¨DistributedSampler
        if self.distributed:
            train_sampler = DistributedSampler(train_dataset, num_replicas=self.world_size, rank=self.rank)
            val_sampler = DistributedSampler(val_dataset, num_replicas=self.world_size, rank=self.rank)
            
            train_dataloader = DataLoader(
                train_dataset, 
                batch_size=self.batch_size, 
                sampler=train_sampler,
                num_workers=2,  # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ä½¿ç”¨æ›´å°‘çš„worker
                collate_fn=collate_fn,
                pin_memory=True
            )
            val_dataloader = DataLoader(
                val_dataset, 
                batch_size=self.batch_size, 
                sampler=val_sampler,
                num_workers=2,
                collate_fn=collate_fn,
                pin_memory=True
            )
        else:
            train_dataloader = DataLoader(
                train_dataset, 
                batch_size=self.batch_size, 
                shuffle=True, 
                num_workers=0,
                collate_fn=collate_fn
            )
            val_dataloader = DataLoader(
                val_dataset, 
                batch_size=self.batch_size, 
                shuffle=False, 
                num_workers=0,
                collate_fn=collate_fn
            )
        
        return train_dataloader, val_dataloader

    def visualize_model_output(self, num_images=5):
        self.generator.eval()
        with torch.no_grad():
            z = torch.randn(num_images, self.latent_dim, 1, 1).to(self.device)
            fake_images = self.generator(z)
            fake_images = fake_images.cpu().detach()
            fig, axs = plt.subplots(1, num_images, figsize=(num_images * 3, 3))
            for i, img in enumerate(fake_images):
                axs[i].imshow(img.permute(1, 2, 0))
                axs[i].axis('off')
            plt.show()
            plt.close()

    def visualize_weights(self):
        for name, param in self.generator.named_parameters():
            if 'weight' in name:
                plt.figure(figsize=(15, 5))
                plt.title(f'{name} weights')
                plt.hist(param.data.cpu().numpy().flatten(), bins=100)
                plt.show()
                plt.close()

        for name, param in self.discriminator.named_parameters():
            if 'weight' in name:
                plt.figure(figsize=(15, 5))
                plt.title(f'{name} weights')
                plt.hist(param.data.cpu().numpy().flatten(), bins=100)
                plt.show()
                plt.close()

    def train(self):
        """è®­ç»ƒæ–¹æ³•ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ"""
        start_time = time.time()
        
        if is_main_process():
            self.logger.debug(f'{safe_emoji("ğŸš€", "[START]")} å¼€å§‹è®­ç»ƒçš„æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}')

        # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºå­˜å‚¨ç”Ÿæˆå›¾ç‰‡çš„ç›®å½•
        if is_main_process() and not os.path.exists(os.path.join(self.train_epoch_dir, 'fake_images_new')):
            os.makedirs(os.path.join(self.train_epoch_dir, 'fake_images_new'))

        real_label = 1.0
        fake_label = 0.0
        fixed_noise = torch.randn(64, self.latent_dim, 1, 1, device=self.device)
        img_list = []
        G_losses = []
        D_losses = []
        iters = 0
        early_stopping_counter = 0

        if is_main_process():
            self.logger.debug(f'{safe_emoji("ğŸ”¥", "")}{"<" * 30}å¼€å§‹è®­ç»ƒ{">" * 30}{safe_emoji("ğŸ”¥", "")}')
            
        self.generator.train()
        self.discriminator.train()
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        start_epoch = self._load_models_unified()
        self.start_epoch = start_epoch
        
        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if self.distributed:
            dist.barrier()
        
        try:
            for epoch in range(start_epoch, self.epochs):
                # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œè®¾ç½®samplerçš„epoch
                if self.distributed:
                    self.train_dataloader.sampler.set_epoch(epoch)
                    
                for i, data in enumerate(self.train_dataloader, 0):
                    real_cpu = data[0].to(self.device)
                    b_size = real_cpu.size(0)
                    
                    ############################
                    # (1) è®­ç»ƒåˆ¤åˆ«å™¨ï¼šæœ€å¤§åŒ– log(D(x)) + log(1 - D(G(z)))
                    ###########################
                    self.discriminator.zero_grad()
                    
                    # è®­ç»ƒçœŸå®æ•°æ®
                    label = torch.full((b_size,), real_label, dtype=torch.float, device=self.device)
                    
                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                        output = self.discriminator(real_cpu).view(-1)
                        errD_real = self.criterion(output, label)
                    
                    # æ··åˆç²¾åº¦è®­ç»ƒçš„æ­£ç¡®æµç¨‹
                    self.scaler.scale(errD_real).backward()
                    D_x = output.mean().item()

                    # è®­ç»ƒç”Ÿæˆçš„å‡æ•°æ®
                    noise = torch.randn(b_size, self.latent_dim, 1, 1, device=self.device)
                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                        fake = self.generator(noise)
                        label.fill_(fake_label)
                        output = self.discriminator(fake.detach()).view(-1)
                        errD_fake = self.criterion(output, label)
                    
                    self.scaler.scale(errD_fake).backward()
                    D_G_z1 = output.mean().item()
                    errD = errD_real + errD_fake
                    
                    # æ¢¯åº¦ç´¯ç§¯å¤„ç†
                    if (i + 1) % self.gradient_accumulation_steps == 0:
                        self.scaler.step(self.optimizer_D)
                        self.scaler.update()
                        self.discriminator.zero_grad()

                    ############################
                    # (2) è®­ç»ƒç”Ÿæˆå™¨ï¼šæœ€å¤§åŒ– log(D(G(z)))
                    ###########################
                    self.generator.zero_grad()
                    label.fill_(real_label)  # ç”Ÿæˆå™¨å¸Œæœ›åˆ¤åˆ«å™¨è®¤ä¸ºå‡æ•°æ®æ˜¯çœŸçš„
                    
                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                        output = self.discriminator(fake).view(-1)
                        errG = self.criterion(output, label)
                    
                    self.scaler.scale(errG).backward()
                    D_G_z2 = output.mean().item()

                    # æ¢¯åº¦ç´¯ç§¯å¤„ç†
                    if (i + 1) % self.gradient_accumulation_steps == 0:
                        self.scaler.step(self.optimizer_G)
                        self.scaler.update()
                        self.generator.zero_grad()

                    # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºè®­ç»ƒæ—¥å¿—
                    if i % 50 == 0 and is_main_process():
                        self.logger.debug('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                                     % (epoch, self.epochs, i, len(self.train_dataloader),
                                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

                    G_losses.append(errG.item())
                    D_losses.append(errD.item())

                    # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ä¸­é—´å›¾åƒ
                    if is_main_process() and ((iters % 500 == 0) or ((epoch == self.epochs - 1) and (i == len(self.train_dataloader) - 1))):
                        with torch.no_grad():
                            fake = self.generator(fixed_noise.to(torch.float32)).detach().cpu()
                        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

                    iters += 1

                # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                if self.distributed:
                    dist.barrier()

                #åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæ¯ä¸ªepochç»“æŸæ—¶ï¼Œè°ƒç”¨save_stateæ–¹æ³•æ¥ä¿å­˜çŠ¶æ€ï¼š
                self.save_state(epoch)

                # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
                if is_main_process():
                    with torch.no_grad():
                        fake = self.generator(fixed_noise).detach().cpu()
                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))
                    save_image(fake.data,os.path.join(self.train_epoch_dir, f'fake_images_new/fake_images_epoch_{epoch}_Loss_D_{errD.item():.4f}_Loss_G_{errG.item():.4f}_D_x_{D_x}_D_x_{(D_G_z1/D_G_z2):.4f}.png'), normalize=True)

                # åœ¨è®­ç»ƒå¾ªç¯ç»“æŸåï¼Œæ·»åŠ ä»¥ä¸‹ä»£ç ä»¥æ‰“å°ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„æŸå¤±å¹¶å¯è§†åŒ–ç”Ÿæˆçš„å›¾åƒ
                self.generator.eval()
                self.discriminator.eval()

                # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºå›¾åƒ
                if epoch % 50 == 0 and is_main_process():
                    # å°†ç”Ÿæˆçš„å›¾åƒæ˜¾ç¤ºåœ¨æ§åˆ¶å°
                    plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True).cpu(), (1, 2, 0)))
                    plt.axis('off')
                    #plt.show()

                    # ç»˜åˆ¶ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„æŸå¤±æ›²çº¿
                    plt.figure(figsize=(10, 5))
                    plt.title(f"Generator and Discriminator Loss After Epoch {epoch}")
                    plt.plot(G_losses, label="Generator Loss")
                    plt.plot(D_losses, label="Discriminator Loss")
                    plt.xlabel("Iterations")
                    plt.ylabel("Loss")
                    plt.legend()
                    #plt.show()

                # ä¿®æ­£éªŒè¯é€»è¾‘ï¼šä½¿ç”¨validation_frequencyè€Œä¸æ˜¯patienceä½œä¸ºéªŒè¯é¢‘ç‡
                if epoch % self.validation_frequency == 0:
                    self.generator.eval()
                    val_loss = 0
                    
                    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªåœ¨ä¸»è¿›ç¨‹è¿›è¡ŒéªŒè¯
                    if is_main_process():
                        with torch.no_grad():
                            for i, data in enumerate(self.val_dataloader, 0):
                                real_cpu = data[0].to(self.device)
                                b_size = real_cpu.size(0)
                                label = torch.full((b_size,), real_label, dtype=torch.float, device=self.device)
                                output = self.discriminator(real_cpu).view(-1)
                                errD_real = self.criterion(output, label)
                                D_x = output.mean().item()

                                noise = torch.randn(b_size, self.latent_dim, 1, 1, device=self.device)
                                fake = self.generator(noise)
                                label.fill_(fake_label)
                                output = self.discriminator(fake.detach()).view(-1)
                                errD_fake = self.criterion(output, label)
                                D_G_z1 = output.mean().item()
                                errD = errD_real + errD_fake

                                label.fill_(real_label)
                                output = self.discriminator(fake).view(-1)
                                errG = self.criterion(output, label)
                                D_G_z2 = output.mean().item()

                                val_loss += errG.item() + errD.item()

                        val_loss /= len(self.val_dataloader)
                        self.logger.debug(f'è®­ç»ƒåˆ°ç¬¬{epoch}ä¸ªå‘¨æœŸåçš„éªŒè¯æŸå¤±ä¸º: {val_loss}')

                        # æ—©åœé€»è¾‘ï¼šå¦‚æœéªŒè¯æŸå¤±æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
                        if val_loss < self.best_val_loss - self.min_delta:
                            self.best_val_loss = val_loss
                            early_stopping_counter = 0

                            self.generator.train()
                            self.discriminator.train()

                            # åœ¨æ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜æ­¤æ—¶çš„æ¨¡å‹æ–‡ä»¶
                            # ä¿å­˜ç”Ÿæˆå™¨çš„æ¨¡å‹
                            torch.save({
                                'epoch': epoch,
                                'generator_state_dict': self.generator.state_dict(),
                                'optimizer_G_state_dict': self.optimizer_G.state_dict(),
                                'scheduler_G_state_dict': self.scheduler_G.state_dict(),
                                'best_val_loss': self.best_val_loss,
                                'val_loss': val_loss,
                                'early_stopping_counter': early_stopping_counter,
                            },os.path.join(self.train_epoch_dir, f'generator_all_epoch_{start_epoch}to{self.epochs}.pth'))

                            # ä¿å­˜åˆ¤åˆ«å™¨çš„æ¨¡å‹
                            torch.save({
                                'epoch': epoch,
                                'discriminator_state_dict': self.discriminator.state_dict(),
                                'optimizer_D_state_dict': self.optimizer_D.state_dict(),
                                'scheduler_D_state_dict': self.scheduler_D.state_dict(),
                                'best_val_loss': self.best_val_loss,
                                'val_loss': val_loss,
                                'early_stopping_counter': early_stopping_counter,
                            },os.path.join(self.train_epoch_dir, f'discriminator_all_epoch_{start_epoch}to{self.epochs}.pth'))

                        else:
                            early_stopping_counter += 1
                            self.logger.debug(f'è®­ç»ƒåˆ°ç¬¬{epoch}ä¸ªå‘¨æœŸåæ—©åœæ¬¡æ•°+1ï¼Œå½“å‰æ—©åœæ¬¡æ•°ä¸ºï¼š{early_stopping_counter}.å› ä¸ºéªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„ï¼Œæˆ–è€…æ”¹å–„çš„å¹…åº¦å°äºmin_delta')
                            if early_stopping_counter >= self.patience:
                                self.logger.debug(
                                    f'è®­ç»ƒåˆ°ç¬¬{epoch}ä¸ªå‘¨æœŸååœæ­¢è®­ç»ƒï¼Œå› ä¸ºæ—©åœæ¬¡æ•°å¤§äºç­‰äºè€å¿ƒå€¼ï¼š{self.patience}ä¸”éªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„ï¼Œæˆ–è€…æ”¹å–„çš„å¹…åº¦å°äºmin_delta')
                                break
                    
                    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                    if self.distributed:
                        dist.barrier()

        except KeyboardInterrupt:
            if is_main_process():
                self.logger.info(f'\n{safe_emoji("ğŸ›‘", "[STOP]")} ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒ (Epoch {epoch}) {safe_emoji("ğŸ›‘", "[STOP]")}')
                self.logger.info(f'{safe_emoji("ğŸ’¾", "[SAVE]")} æ­£åœ¨ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€...')
            
            # ä¿å­˜å½“å‰çŠ¶æ€
            self.save_state(epoch)
            
            # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ç®€å•æ¨¡å‹
            if is_main_process():
                torch.save(self.generator.state_dict(), os.path.join(self.train_epoch_dir,f'generator_simple_epoch_{start_epoch}to{epoch}_interrupted.pth'))
                torch.save(self.discriminator.state_dict(), os.path.join(self.train_epoch_dir,f'discriminator_simple_epoch_{start_epoch}to{epoch}_interrupted.pth'))
                self.logger.info(f'{safe_emoji("âœ…", "[SUCCESS]")} è®­ç»ƒçŠ¶æ€å·²ä¿å­˜ï¼å¯ä»¥é€šè¿‡è®¾ç½®start_epoch={epoch+1}æ¥æ¢å¤è®­ç»ƒ')
            return G_losses, D_losses, img_list

        if is_main_process():
            self.logger.debug(f'{safe_emoji("ğŸ‰", "[COMPLETE]")} è®­ç»ƒç»“æŸï¼')

        # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹
        if is_main_process():
            torch.save(self.generator.state_dict(), os.path.join(self.train_epoch_dir,f'generator_simple_epoch_{start_epoch}to{self.epochs}.pth'))
            torch.save(self.discriminator.state_dict(), os.path.join(self.train_epoch_dir,f'discriminator_simple_epoch_{start_epoch}to{self.epochs}.pth'))

        # åªåœ¨ä¸»è¿›ç¨‹ç»˜åˆ¶æŸå¤±æ›²çº¿
        if is_main_process():
            # ç»˜åˆ¶æŸå¤±æ›²çº¿
            plt.figure(figsize=(10, 5))
            plt.title(f"Generator and Discriminator Loss During Training Between {start_epoch}To{self.epochs} Epoch \nè®­ç»ƒæœŸé—´ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æŸè€—")
            plt.plot(G_losses, label="Gç”Ÿæˆå™¨")
            plt.plot(D_losses, label="Dåˆ¤åˆ«å™¨")
            plt.xlabel("iterationsè¿­ä»£")
            plt.ylabel("LossæŸè€—")
            plt.legend()
            plt.show()

            # åŠ¨æ€å±•ç¤ºç”Ÿæˆå›¾ç‰‡çš„è¿‡ç¨‹
            # åœ¨åˆ›å»ºåŠ¨ç”»ä¹‹å‰è®¾ç½®embed_limit
            plt.rcParams['animation.embed_limit'] = 30.0  # æˆ–è€…è®¾ç½®é€‚åˆä½ éœ€æ±‚çš„å€¼

            fig = plt.figure(figsize=(8, 8))
            plt.axis("off")
            ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]
            ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)
            HTML(ani.to_jshtml())

            #å¦å¤–ä¿å­˜ç”Ÿæˆå›¾ç‰‡çš„è¿‡ç¨‹åŠ¨ç”»ä¸ºhtmlæ–‡ä»¶

            '''
            å®ƒé¦–å…ˆä½¿ç”¨to_jshtmlæ–¹æ³•å°†åŠ¨ç”»è½¬æ¢ä¸ºä¸€ä¸ªHTMLå­—ç¬¦ä¸²ï¼Œç„¶åå°†è¿™ä¸ªå­—ç¬¦ä¸²å†™å…¥åˆ°ä¸€ä¸ªåä¸ºanimation.htmlçš„æ–‡ä»¶ä¸­ã€‚
            ä½ å¯ä»¥åœ¨ä»»ä½•æµè§ˆå™¨ä¸­æ‰“å¼€è¿™ä¸ªæ–‡ä»¶æ¥æŸ¥çœ‹åŠ¨ç”»ã€‚
            '''

            html = ani.to_jshtml()
            with open(os.path.join(self.train_epoch_dir,f'animation_epoch_{self.start_epoch}to{self.epochs}.html'), 'w') as f:
                f.write(html)

            # å¯è§†åŒ–æ¨¡å‹è¾“å‡º
            self.visualize_model_output()
            # å¯è§†åŒ–æƒé‡
            self.visualize_weights()

            self.logger.debug(f'{safe_emoji("ğŸ”¥", "")}{"<" * 30}è®­ç»ƒç»“æŸ{">" * 30}{safe_emoji("ğŸ”¥", "")}')

            # è®¡ç®—è€—æ—¶
            end_time = time.time()
            self.logger.debug(f'{safe_emoji("ğŸ", "[END]")} ç»“æŸè®­ç»ƒçš„æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}')
            diff_time = end_time - start_time
            hours = int(diff_time // 3600)
            minutes = int((diff_time % 3600) // 60)
            seconds = int(diff_time % 60)
            self.logger.debug(f'{safe_emoji("â±ï¸", "[TIME]")} è®­ç»ƒæ€»è€—æ—¶ï¼š{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’')
        
        return G_losses, D_losses, img_list

def launch_distributed_training(dataset_path, num_gpus=None, **kwargs):
    """
    å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
    
    å‚æ•°:
        dataset_path: æ•°æ®é›†è·¯å¾„
        num_gpus: ä½¿ç”¨çš„GPUæ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
        **kwargs: ä¼ é€’ç»™GANTrainerçš„å…¶ä»–å‚æ•°
    """
    import torch.multiprocessing as mp
    
    if num_gpus is None:
        num_gpus = torch.cuda.device_count()
    
    if num_gpus <= 1:
        # å•GPUæˆ–CPUè®­ç»ƒ
        trainer = GANTrainer(dataset_path=dataset_path, **kwargs)
        return trainer.train()
    else:
        # å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
        print(f"ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œä½¿ç”¨ {num_gpus} ä¸ªGPU")
        mp.spawn(
            _distributed_train_worker,
            args=(num_gpus, dataset_path, kwargs),
            nprocs=num_gpus,
            join=True
        )


def _distributed_train_worker(rank, world_size, dataset_path, kwargs):
    """
    åˆ†å¸ƒå¼è®­ç»ƒå·¥ä½œè¿›ç¨‹
    """
    try:
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        setup_distributed(rank, world_size)
        
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®¾ç½®åˆ†å¸ƒå¼å‚æ•°
        trainer = GANTrainer(
            dataset_path=dataset_path,
            distributed=True,
            rank=rank,
            world_size=world_size,
            **kwargs
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
    except Exception as e:
        if is_main_process():
            print(f"åˆ†å¸ƒå¼è®­ç»ƒå‡ºé”™: {e}")
        raise e
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()


if __name__ == '__main__':
    # ä¿®æ”¹ç¡¬ç¼–ç è·¯å¾„ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ä»ç¯å¢ƒå˜é‡è·å–
    dataset_path = os.getenv('DATASET_PATH', '/kaggle/input/silentpod-mulit-sizes/64x64-1024x1024all')  # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œé»˜è®¤ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    use_distributed = os.getenv('USE_DISTRIBUTED', 'false').lower() == 'true'
    num_gpus = int(os.getenv('NUM_GPUS', '0')) if os.getenv('NUM_GPUS') else None
    
    if use_distributed:
        # å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
        launch_distributed_training(
            dataset_path=dataset_path,
            num_gpus=num_gpus,
            num_layers=4,
            base_channels=64,
            load_models=True,
            epochs=2000,
            batch_size=256,
            patience=500,
            validation_frequency=10
        )
    else:
        # å•æœºè®­ç»ƒ
        trainer = GANTrainer(
            dataset_path=dataset_path, 
            num_layers=4, 
            base_channels=64,
            load_models=True, 
            epochs=2000,
            batch_size=256,
            patience=500,
            validation_frequency=10  # æ–°å¢å‚æ•°ï¼šæ¯10ä¸ªepochéªŒè¯ä¸€æ¬¡
        )
        trainer.train()
