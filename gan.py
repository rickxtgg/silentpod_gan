# -*- coding: utf-8 -*-
"""
GANè®­ç»ƒå™¨ - æ”¯æŒemojiç¬¦å·å’ŒUnicodeå­—ç¬¦ + DDPåˆ†å¸ƒå¼è®­ç»ƒ + GPUæ•°æ®å¤„ç†ä¼˜åŒ–
ä½¿ç”¨UTF-8ç¼–ç ç¡®ä¿æ‰€æœ‰å­—ç¬¦æ­£ç¡®æ˜¾ç¤ºå’Œä¿å­˜
æ”¯æŒå•æœºå¤šå¡å’Œå¤šæœºå¤šå¡çš„åˆ†å¸ƒå¼è®­ç»ƒ
ä¼˜åŒ–GPUæ•°æ®å¤„ç†ç®¡é“ï¼Œæå‡è®­ç»ƒæ•ˆç‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from torchvision.utils import save_image
import torchvision.utils as vutils
from torch.utils.data import random_split
from torchvision import transforms
import torchvision.transforms.functional as TF
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import animation
from torch.amp import GradScaler, autocast  # ä½¿ç”¨æ–°çš„API
from IPython.display import HTML
import os
import time
import logging
import logging.config
import argparse

def setup_utf8_environment():
    """è®¾ç½®UTF-8ç¯å¢ƒï¼Œç¡®ä¿emojiç¬¦å·èƒ½æ­£ç¡®æ˜¾ç¤º"""
    import sys
    
    # ä¸ºWindowsè®¾ç½®UTF-8ç¯å¢ƒ
    if os.name == 'nt':  # Windowsç³»ç»Ÿ
        try:
            # è®¾ç½®æ§åˆ¶å°ä»£ç é¡µä¸ºUTF-8
            os.system('chcp 65001 > nul')
            
            # é‡æ–°é…ç½®æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
            if hasattr(sys.stdout, 'reconfigure'):
                sys.stdout.reconfigure(encoding='utf-8', errors='replace')
            if hasattr(sys.stderr, 'reconfigure'):
                sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•è®¾ç½®UTF-8ç¯å¢ƒ: {e}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTHONIOENCODING'] = 'utf-8'

def safe_emoji(emoji_text, fallback_text):
    """å®‰å…¨çš„emojiæ˜¾ç¤ºå‡½æ•°ï¼Œåœ¨ä¸æ”¯æŒçš„ç¯å¢ƒä¸­ä½¿ç”¨æ›¿ä»£æ–‡æœ¬"""
    try:
        # æµ‹è¯•æ˜¯å¦èƒ½ç¼–ç emoji
        emoji_text.encode('utf-8')
        return emoji_text
    except UnicodeEncodeError:
        return fallback_text

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
        torch.cuda.set_device(local_rank)
        
        return True, rank, world_size, local_rank
    else:
        return False, 0, 1, 0

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()

def is_main_process():
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0

# åœ¨æ¨¡å—åŠ è½½æ—¶ç«‹å³è®¾ç½®UTF-8ç¯å¢ƒ
setup_utf8_environment()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class GPUDataProcessor:
    """GPUæ•°æ®å¤„ç†ç±»ï¼Œä¸“é—¨å¤„ç†GPUä¸Šçš„æ•°æ®å˜æ¢å’Œé¢„å¤„ç†"""
    
    def __init__(self, device, image_size=64):
        self.device = device
        self.image_size = image_size
        
    def process_batch_on_gpu(self, images):
        """åœ¨GPUä¸Šæ‰¹é‡å¤„ç†å›¾åƒæ•°æ®"""
        with torch.no_grad():
            # ç¡®ä¿æ•°æ®åœ¨GPUä¸Š
            if not images.is_cuda and self.device.type == 'cuda':
                images = images.to(self.device, non_blocking=True)
            
            # GPUä¸Šçš„æ ‡å‡†åŒ–å¤„ç†
            images = images.float() / 255.0  # è½¬æ¢ä¸º[0,1]èŒƒå›´
            images = (images - 0.5) / 0.5    # æ ‡å‡†åŒ–åˆ°[-1,1]èŒƒå›´
            
            return images
    
    def augment_batch_on_gpu(self, images, training=True):
        """åœ¨GPUä¸Šè¿›è¡Œæ•°æ®å¢å¼º"""
        if not training:
            return images
            
        with torch.no_grad():
            batch_size = images.size(0)
            
            # éšæœºæ°´å¹³ç¿»è½¬ï¼ˆ50%æ¦‚ç‡ï¼‰
            flip_mask = torch.rand(batch_size, device=self.device) < 0.5
            for i in range(batch_size):
                if flip_mask[i]:
                    images[i] = torch.flip(images[i], dims=[2])  # æ°´å¹³ç¿»è½¬
            
            # è½»å¾®çš„éšæœºæ—‹è½¬ï¼ˆÂ±5åº¦ï¼‰
            if torch.rand(1).item() < 0.3:  # 30%æ¦‚ç‡è¿›è¡Œæ—‹è½¬
                angle = (torch.rand(1) - 0.5) * 10  # -5åˆ°5åº¦
                # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ—‹è½¬ï¼Œå¯¹äºæ›´å¤æ‚çš„å˜æ¢å¯èƒ½éœ€è¦ä½¿ç”¨korniaåº“
                
            return images

def collate_fn_gpu_accelerated(batch):
    """GPUåŠ é€Ÿçš„æ‰¹é‡æ•°æ®æ•´ç†å‡½æ•°ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’ŒGPUé¢„å¤„ç†"""
    images, labels = zip(*batch)
    
    # ç¡®å®šè®¾å¤‡
    if dist.is_initialized():
        device = torch.cuda.current_device()
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å°†å›¾åƒæ•°æ®å †å å¹¶è½¬ç§»åˆ°GPU
    # è¿™é‡Œimageså¯èƒ½æ˜¯PILå›¾åƒæˆ–tensorï¼Œéœ€è¦ç»Ÿä¸€å¤„ç†
    processed_images = []
    for img in images:
        if isinstance(img, torch.Tensor):
            processed_images.append(img)
        else:
            # å¦‚æœæ˜¯PILå›¾åƒï¼Œè½¬æ¢ä¸ºtensor
            img_tensor = TF.to_tensor(img)
            processed_images.append(img_tensor)
    
    # æ‰¹é‡ç§»åŠ¨åˆ°GPUï¼Œä½¿ç”¨non_blocking=TrueåŠ é€Ÿ
    images_batch = torch.stack(processed_images).to(device, non_blocking=True)
    labels_batch = torch.tensor(labels, device=device, dtype=torch.long)
    
    return images_batch, labels_batch

class MyDatasetGPU(Dataset):
    """ä¼˜åŒ–çš„æ•°æ®é›†ç±»ï¼Œæ”¯æŒGPUé¢„å¤„ç†å’Œæ›´é«˜æ•ˆçš„æ•°æ®åŠ è½½"""
    
    def __init__(self, root_dir, image_size=64, use_gpu_transform=True, device=None):
        self.root_dir = root_dir
        self.image_size = image_size
        self.use_gpu_transform = use_gpu_transform
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ‰«æå›¾åƒæ–‡ä»¶
        self.image_files = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
            self.image_files.extend([f for f in os.listdir(root_dir) if f.lower().endswith(ext)])
        
        # é¢„å…ˆå®šä¹‰CPUå˜æ¢ï¼ˆæœ€å°åŒ–çš„ï¼‰
        self.cpu_transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.ToTensor(),  # è½¬æ¢ä¸ºtensorï¼Œå€¼åŸŸ[0,1]
        ])
        
        # GPUæ•°æ®å¤„ç†å™¨
        self.gpu_processor = GPUDataProcessor(self.device, self.image_size)
        
        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.image_files)} å¼ å›¾ç‰‡, GPUå¤„ç†: {self.use_gpu_transform}")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.image_files[idx])
        
        try:
            # ä½¿ç”¨PILåŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            
            # åº”ç”¨CPUé¢„å¤„ç†ï¼ˆresizeå’Œè½¬tensorï¼‰
            if self.cpu_transform:
                image = self.cpu_transform(image)
            
            # è¿”å›tensorå’Œæ ‡ç­¾
            return image, 0  # è¿™é‡Œæ ‡ç­¾ç”¨0å ä½ï¼ŒGANè®­ç»ƒä¸­ä¸éœ€è¦çœŸå®æ ‡ç­¾
            
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥ {img_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»‘è‰²å›¾åƒä½œä¸ºfallback
            fallback_image = torch.zeros((3, self.image_size, self.image_size))
            return fallback_image, 0

class DataLoaderGPU:
    """GPUä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨åŒ…è£…ç±»"""
    
    def __init__(self, dataset, batch_size, shuffle=True, num_workers=4, 
                 pin_memory=True, drop_last=True, device=None, is_distributed=False,
                 sampler=None):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.is_distributed = is_distributed
        self.gpu_processor = GPUDataProcessor(self.device)
        
        # åˆ›å»ºæ ‡å‡†DataLoader
        self.dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle if sampler is None else False,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=drop_last,
            collate_fn=collate_fn_gpu_accelerated,
            sampler=sampler,
            persistent_workers=num_workers > 0  # ä¿æŒworkerè¿›ç¨‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
        )
    
    def __iter__(self):
        for batch_images, batch_labels in self.dataloader:
            # åœ¨GPUä¸Šè¿›è¡Œæœ€ç»ˆçš„æ•°æ®å¤„ç†
            if self.device.type == 'cuda':
                # åº”ç”¨GPUæ•°æ®å¢å¼ºå’Œæ ‡å‡†åŒ–
                batch_images = self.gpu_processor.process_batch_on_gpu(batch_images)
                batch_images = self.gpu_processor.augment_batch_on_gpu(batch_images, training=True)
            
            yield batch_images, batch_labels
    
    def __len__(self):
        return len(self.dataloader)
    
    @property
    def sampler(self):
        return self.dataloader.sampler

class Generator(nn.Module):
    def __init__(self, latent_dim, num_layers, base_channels):
        super(Generator, self).__init__()
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.ConvTranspose2d(latent_dim, base_channels * 8, 4, 1, 0, bias=False))
            else:
                layers.append(nn.ConvTranspose2d(base_channels * (8 // (2 ** (i - 1))), base_channels * (8 // (2 ** i)), 4, 2, 1, bias=False))
            layers.append(nn.BatchNorm2d(base_channels * (8 // (2 ** i))))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
        layers.append(nn.ConvTranspose2d(base_channels, 3, 4, 2, 1, bias=False))
        layers.append(nn.Tanh())
        self.main = nn.Sequential(*layers)


    def forward(self, input):
        return self.main(input.to(torch.float32))# å°†æƒé‡ç±»å‹è®¾ç½®ä¸ºå•ç²¾åº¦æµ®ç‚¹æ•°

class Discriminator(nn.Module):
    def __init__(self, num_layers, base_channels):
        super(Discriminator, self).__init__()
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.Conv2d(3, base_channels, 4, 2, 1, bias=False))
            else:
                layers.append(nn.Conv2d(base_channels * (2 ** (i - 1)), base_channels * (2 ** i), 4, 2, 1, bias=False))
                layers.append(nn.BatchNorm2d(base_channels * (2 ** i)))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
        layers.append(nn.Conv2d(base_channels * (2 ** (num_layers - 1)), 1, 4, 1, 0, bias=False))
        self.main = nn.Sequential(*layers)

    def forward(self, input):
        return self.main(input.to(torch.float32))

class GANTrainer:
    def __init__(self, dataset_path, latent_dim=100, lr_G=0.0002, lr_D=0.0002, betas=(0.5, 0.999), batch_size=128, image_size=64,
                 epochs=50,start_epoch=0,patience=5, min_delta=0.0001, num_layers=4, base_channels=64, load_models=False,gradient_accumulation_steps=1, validation_frequency=10):
        
        # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
        self.is_distributed, self.rank, self.world_size, self.local_rank = setup_distributed()
        
        self.dataset_path = dataset_path
        self.latent_dim = latent_dim
        self.lr_G = lr_G
        self.lr_D = lr_D
        self.betas = betas
        self.batch_size = batch_size
        self.image_size = image_size
        self.epochs = epochs
        self.start_epoch=start_epoch
        self.patience = patience
        self.min_delta = min_delta
        self.num_layers = num_layers
        self.base_channels = base_channels
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.validation_frequency = validation_frequency  # æ–°å¢éªŒè¯é¢‘ç‡å‚æ•°ï¼Œä¸patienceè¯­ä¹‰åˆ†ç¦»
        self.load_models = load_models  # ä¿å­˜åŠ è½½æ¨¡å‹æ ‡å¿—
        self.best_val_loss = float('inf')  # è®¾ç½®ä¸€ä¸ªåˆå§‹å€¼ï¼Œä¾‹å¦‚æ­£æ— ç©·å¤§

        # è®¾ç½®è®¾å¤‡
        if self.is_distributed:
            self.device = torch.device(f'cuda:{self.local_rank}')
            torch.cuda.set_device(self.local_rank)
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç»Ÿä¸€Loggerç®¡ç† - åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºlogger
        if is_main_process():
            self.logger = self._setup_logger()
            self.logger.debug(f'{safe_emoji("ğŸ’»", "[DEVICE]")} å½“å‰ä½¿ç”¨çš„è®¾å¤‡æ˜¯ï¼š{self.device}')
            if self.is_distributed:
                self.logger.info(f'{safe_emoji("ğŸŒ", "[DDP]")} åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼ï¼šRank {self.rank}/{self.world_size}, Local Rank: {self.local_rank}')
            
            # GPUæ€§èƒ½ä¿¡æ¯æ—¥å¿—
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(self.device)
                gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1024**3
                self.logger.info(f'{safe_emoji("ğŸ”¥", "[GPU]")} GPUè®¾å¤‡: {gpu_name}')
                self.logger.info(f'{safe_emoji("ğŸ’¾", "[MEMORY]")} GPUæ˜¾å­˜: {gpu_memory:.1f} GB')
                
                # è®¾ç½®GPUä¼˜åŒ–é€‰é¡¹
                torch.backends.cudnn.benchmark = True  # å¯ç”¨cuDNNä¼˜åŒ–
                torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æ“ä½œä»¥æå‡æ€§èƒ½
                
                # è®¾ç½®å†…å­˜æ± ä¼˜åŒ–
                if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                    # ä¸ºè®­ç»ƒé¢„ç•™è¶³å¤Ÿçš„æ˜¾å­˜ï¼Œé¿å…OOM
                    torch.cuda.set_per_process_memory_fraction(0.95)
                
                self.logger.info(f'{safe_emoji("âš¡", "[OPTIMIZE]")} GPUä¼˜åŒ–è®¾ç½®å·²å¯ç”¨')
        else:
            self.logger = None

        self.generator = Generator(latent_dim, num_layers, base_channels).to(self.device)
        self.discriminator = Discriminator(num_layers, base_channels).to(self.device)

        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥BNå±‚
        if self.is_distributed:
            self.generator = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.generator)
            self.discriminator = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.discriminator)

        self.criterion = nn.BCEWithLogitsLoss()
        self.optimizer_G = torch.optim.AdamW(self.generator.parameters(), lr=self.lr_G, betas=betas)
        self.optimizer_D = torch.optim.AdamW(self.discriminator.parameters(), lr=self.lr_D, betas=betas)

        self.scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_G, T_max=self.epochs)
        self.scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_D, T_max=self.epochs)

        # ä¸ºDDPè®¾ç½®GradScaler
        if self.is_distributed:
            self.scaler = GradScaler('cuda')
        else:
            self.scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')

        self.dataset = self.load_dataset()
        self.train_dataloader, self.val_dataloader = self.split_dataset()
        
        # åŒ…è£…æ¨¡å‹ä¸ºDDP
        if self.is_distributed:
            self.generator = DDP(self.generator, device_ids=[self.local_rank], output_device=self.local_rank, find_unused_parameters=True)
            self.discriminator = DDP(self.discriminator, device_ids=[self.local_rank], output_device=self.local_rank, find_unused_parameters=True)
        
        plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

        self.train_epoch_dir = 'train_epoch'  # Specify the directory for saving files

        # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•
        if is_main_process() and not os.path.exists(self.train_epoch_dir):
            os.makedirs(self.train_epoch_dir)

    def _setup_logger(self):
        """ç»Ÿä¸€è®¾ç½®Loggeré…ç½®"""
        module_name = 'YPS'
        logger = logging.getLogger(f'{__name__}')
        
        # å¦‚æœloggerå·²ç»æœ‰handlerï¼Œå…ˆæ¸…ç©ºé¿å…é‡å¤
        if logger.handlers:
            logger.handlers.clear()
            
        logger.setLevel(logging.DEBUG)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists('train_epoch'):
            os.makedirs('train_epoch')

        # ä½¿ç”¨UTF-8ç¼–ç æ¥æ”¯æŒemojiç¬¦å·
        fh = logging.FileHandler(
            os.path.join('train_epoch', f"{module_name}_è®­ç»ƒlog.txt"), 
            encoding='utf-8'
        )
        fh.setLevel(logging.DEBUG)

        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        logger.addHandler(fh)
        logger.addHandler(ch)
        
        return logger

    def _load_models_unified(self):
        """ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½æ–¹æ³•ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼ï¼Œæ”¯æŒDDP"""
        if not self.load_models:
            if is_main_process() and self.logger:
                self.logger.debug("æœªå¯ç”¨æ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            return 0
            
        # ä¼˜å…ˆçº§1: å°è¯•åŠ è½½å®Œæ•´çš„checkpoint
        checkpoint_path = os.path.join(self.train_epoch_dir, f'checkpoint_epoch_{self.start_epoch}to{self.epochs}.pth')
        if os.path.isfile(checkpoint_path):
            try:
                # åœ¨GPUä¸ŠåŠ è½½checkpoint
                map_location = {'cuda:%d' % 0: 'cuda:%d' % self.local_rank} if self.is_distributed else None
                checkpoint = torch.load(checkpoint_path, map_location=map_location)
                
                # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå‰¥ç¦»DDPåŒ…è£…ï¼‰
                generator_model = self.generator.module if self.is_distributed else self.generator
                discriminator_model = self.discriminator.module if self.is_distributed else self.discriminator
                
                generator_model.load_state_dict(checkpoint['generator_state_dict'])
                discriminator_model.load_state_dict(checkpoint['discriminator_state_dict'])
                self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
                self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
                self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
                self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
                
                recovered_epoch = checkpoint['epoch']
                if is_main_process() and self.logger:
                    self.logger.info(f"{safe_emoji('âœ…', '[SUCCESS]')} æˆåŠŸä»checkpointæ¢å¤è®­ç»ƒçŠ¶æ€ï¼Œä¸Šæ¬¡è®­ç»ƒåˆ°ç¬¬{recovered_epoch}ä¸ªepoch")
                    self.logger.debug("ç”Ÿæˆå™¨ç»“æ„ï¼š\n%s", generator_model)
                    self.logger.debug("åˆ¤åˆ«å™¨ç»“æ„ï¼š\n%s", discriminator_model)
                    self.logger.debug("ç”Ÿæˆå™¨å‚æ•°æ•°é‡ï¼š %s", sum(p.numel() for p in generator_model.parameters()))
                    self.logger.debug("åˆ¤åˆ«å™¨å‚æ•°æ•°é‡ï¼š %s", sum(p.numel() for p in discriminator_model.parameters()))
                
                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                if self.is_distributed:
                    dist.barrier()
                
                return recovered_epoch + 1  # è¿”å›ä¸‹ä¸€ä¸ªè¦è®­ç»ƒçš„epoch
                
            except Exception as e:
                if is_main_process() and self.logger:
                    self.logger.warning(f"{safe_emoji('âš ï¸', '[WARNING]')} CheckpointåŠ è½½å¤±è´¥ï¼š{e}")
        
        # ä¼˜å…ˆçº§2: å°è¯•åŠ è½½ç®€å•çš„æ¨¡å‹æ–‡ä»¶
        generator_path = f'generator_simple_epoch_{self.start_epoch}to{self.epochs}.pth'
        discriminator_path = f'discriminator_simple_epoch_{self.start_epoch}to{self.epochs}.pth'
        
        if os.path.isfile(generator_path) and os.path.isfile(discriminator_path):
            try:
                map_location = {'cuda:%d' % 0: 'cuda:%d' % self.local_rank} if self.is_distributed else None
                
                # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå‰¥ç¦»DDPåŒ…è£…ï¼‰
                generator_model = self.generator.module if self.is_distributed else self.generator
                discriminator_model = self.discriminator.module if self.is_distributed else self.discriminator
                
                generator_model.load_state_dict(torch.load(generator_path, map_location=map_location))
                discriminator_model.load_state_dict(torch.load(discriminator_path, map_location=map_location))
                
                if is_main_process() and self.logger:
                    self.logger.info(f"{safe_emoji('âœ…', '[SUCCESS]')} æˆåŠŸåŠ è½½ç®€å•æ¨¡å‹æ–‡ä»¶ï¼Œä»ç¬¬{self.start_epoch}ä¸ªepochå¼€å§‹è®­ç»ƒ")
                    self.logger.debug("ç”Ÿæˆå™¨ç»“æ„ï¼š\n%s", generator_model)
                    self.logger.debug("åˆ¤åˆ«å™¨ç»“æ„ï¼š\n%s", discriminator_model)
                
                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                if self.is_distributed:
                    dist.barrier()
                
                return self.start_epoch
                
            except Exception as e:
                if is_main_process() and self.logger:
                    self.logger.warning(f"{safe_emoji('âš ï¸', '[WARNING]')} ç®€å•æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        
        # ä¼˜å…ˆçº§3: å¯»æ‰¾å…¶ä»–å¯ç”¨çš„checkpointæ–‡ä»¶
        checkpoint_pattern = os.path.join(self.train_epoch_dir, 'checkpoint_epoch_*.pth')
        import glob
        checkpoint_files = glob.glob(checkpoint_pattern)
        if checkpoint_files:
            # é€‰æ‹©æœ€æ–°çš„checkpointæ–‡ä»¶
            latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
            try:
                map_location = {'cuda:%d' % 0: 'cuda:%d' % self.local_rank} if self.is_distributed else None
                checkpoint = torch.load(latest_checkpoint, map_location=map_location)
                
                # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå‰¥ç¦»DDPåŒ…è£…ï¼‰
                generator_model = self.generator.module if self.is_distributed else self.generator
                discriminator_model = self.discriminator.module if self.is_distributed else self.discriminator
                
                generator_model.load_state_dict(checkpoint['generator_state_dict'])
                discriminator_model.load_state_dict(checkpoint['discriminator_state_dict'])
                self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
                self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
                self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
                self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
                
                recovered_epoch = checkpoint['epoch']
                if is_main_process() and self.logger:
                    self.logger.info(f"{safe_emoji('âœ…', '[SUCCESS]')} æ‰¾åˆ°å¹¶åŠ è½½äº†å…¶ä»–checkpointæ–‡ä»¶ï¼š{os.path.basename(latest_checkpoint)}")
                    self.logger.info(f"æ¢å¤åˆ°ç¬¬{recovered_epoch}ä¸ªepoch")
                
                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                if self.is_distributed:
                    dist.barrier()
                
                return recovered_epoch + 1
                
            except Exception as e:
                if is_main_process() and self.logger:
                    self.logger.warning(f"{safe_emoji('âš ï¸', '[WARNING]')} å…¶ä»–checkpointåŠ è½½å¤±è´¥ï¼š{e}")
        
        # æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥
        if is_main_process() and self.logger:
            self.logger.info(f"{safe_emoji('â„¹ï¸', '[INFO]')} æœªæ‰¾åˆ°å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if self.is_distributed:
            dist.barrier()
        
        return 0

    #ä¿å­˜ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ‰€æœ‰æ•°æ®çš„æ¨¡å‹ï¼Œæ”¯æŒDDP
    def save_state(self, epoch):
        """ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼Œæ”¯æŒDDPæ¨¡å‹"""
        if not is_main_process():
            return
            
        checkpoint_path = os.path.join(self.train_epoch_dir, f'checkpoint_epoch_{self.start_epoch}to{self.epochs}.pth')
        
        # è·å–å®é™…çš„æ¨¡å‹çŠ¶æ€ï¼ˆå‰¥ç¦»DDPåŒ…è£…ï¼‰
        generator_state = self.generator.module.state_dict() if self.is_distributed else self.generator.state_dict()
        discriminator_state = self.discriminator.module.state_dict() if self.is_distributed else self.discriminator.state_dict()
        
        torch.save({
            'epoch': epoch,
            'generator_state_dict': generator_state,
            'discriminator_state_dict': discriminator_state,
            'optimizer_G_state_dict': self.optimizer_G.state_dict(),
            'optimizer_D_state_dict': self.optimizer_D.state_dict(),
            'scheduler_G_state_dict': self.scheduler_G.state_dict(),
            'scheduler_D_state_dict': self.scheduler_D.state_dict(),
        }, checkpoint_path)
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if self.is_distributed:
            dist.barrier()

    def load_state(self):
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨å·²ç»è¢«_load_models_unifiedæ›¿ä»£ï¼Œä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§
        checkpoint_path = os.path.join(self.train_epoch_dir, f'checkpoint_epoch_{self.start_epoch}to{self.epochs}.pth')
        if os.path.isfile(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            self.generator.load_state_dict(checkpoint['generator_state_dict'])
            self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
            self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
            self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
            self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
            self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
            return checkpoint['epoch']
        else:
            return None

    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†ï¼Œä½¿ç”¨GPUä¼˜åŒ–çš„æ•°æ®å¤„ç†"""
        # ä¸å†åœ¨è¿™é‡Œå®šä¹‰transformï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨GPUå¤„ç†
        dataset = MyDatasetGPU(
            root_dir=self.dataset_path, 
            image_size=self.image_size,
            use_gpu_transform=True,
            device=self.device
        )
        
        if is_main_process() and self.logger:
            self.logger.info(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} å¼ å›¾ç‰‡")
            self.logger.info(f"ğŸ”§ å›¾åƒå°ºå¯¸: {self.image_size}x{self.image_size}")
            self.logger.info(f"ğŸ’¾ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        return dataset

    def split_dataset(self):
        """æ•°æ®é›†åˆ†å‰²ï¼Œæ”¯æŒåˆ†å¸ƒå¼é‡‡æ ·å’ŒGPUä¼˜åŒ–"""
        train_size = int(0.8 * len(self.dataset))
        val_size = len(self.dataset) - train_size
        train_dataset, val_dataset = random_split(self.dataset, [train_size, val_size])
        
        # è®¡ç®—æœ‰æ•ˆçš„num_workers
        if torch.cuda.is_available():
            # GPUç¯å¢ƒä¸‹ä½¿ç”¨æ›´å¤šworkers
            num_workers = min(4, os.cpu_count() // 2) if not self.is_distributed else 2
        else:
            num_workers = 2
        
        if self.is_distributed:
            # åˆ†å¸ƒå¼é‡‡æ ·å™¨
            train_sampler = DistributedSampler(
                train_dataset, 
                num_replicas=self.world_size, 
                rank=self.rank,
                shuffle=True
            )
            val_sampler = DistributedSampler(
                val_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False
            )
            
            # ä½¿ç”¨GPUä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
            train_dataloader = DataLoaderGPU(
                train_dataset, 
                batch_size=self.batch_size, 
                shuffle=False,  # åˆ†å¸ƒå¼æ—¶ç”±sampleræ§åˆ¶shuffle
                num_workers=num_workers,
                pin_memory=True,
                drop_last=True,
                device=self.device,
                is_distributed=True,
                sampler=train_sampler
            )
            
            val_dataloader = DataLoaderGPU(
                val_dataset, 
                batch_size=self.batch_size, 
                shuffle=False,
                num_workers=num_workers,
                pin_memory=True,
                drop_last=False,
                device=self.device,
                is_distributed=True,
                sampler=val_sampler
            )
        else:
            # éåˆ†å¸ƒå¼è®­ç»ƒ
            train_dataloader = DataLoaderGPU(
                train_dataset, 
                batch_size=self.batch_size, 
                shuffle=True,
                num_workers=num_workers,
                pin_memory=True,
                drop_last=True,
                device=self.device,
                is_distributed=False
            )
            
            val_dataloader = DataLoaderGPU(
                val_dataset, 
                batch_size=self.batch_size, 
                shuffle=False,
                num_workers=num_workers,
                pin_memory=True,
                drop_last=False,
                device=self.device,
                is_distributed=False
            )
        
        if is_main_process() and self.logger:
            self.logger.info(f"ğŸ“ˆ æ•°æ®åˆ†å‰²å®Œæˆ:")
            self.logger.info(f"  - è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
            self.logger.info(f"  - éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
            self.logger.info(f"  - æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            self.logger.info(f"  - å·¥ä½œè¿›ç¨‹æ•°: {num_workers}")
            self.logger.info(f"  - åˆ†å¸ƒå¼è®­ç»ƒ: {self.is_distributed}")
        
        return train_dataloader, val_dataloader

    def visualize_model_output(self, num_images=5):
        self.generator.eval()
        with torch.no_grad():
            z = torch.randn(num_images, self.latent_dim, 1, 1).to(self.device)
            fake_images = self.generator(z)
            fake_images = fake_images.cpu().detach()
            fig, axs = plt.subplots(1, num_images, figsize=(num_images * 3, 3))
            for i, img in enumerate(fake_images):
                axs[i].imshow(img.permute(1, 2, 0))
                axs[i].axis('off')
            plt.show()
            plt.close()

    def visualize_weights(self):
        for name, param in self.generator.named_parameters():
            if 'weight' in name:
                plt.figure(figsize=(15, 5))
                plt.title(f'{name} weights')
                plt.hist(param.data.cpu().numpy().flatten(), bins=100)
                plt.show()
                plt.close()

        for name, param in self.discriminator.named_parameters():
            if 'weight' in name:
                plt.figure(figsize=(15, 5))
                plt.title(f'{name} weights')
                plt.hist(param.data.cpu().numpy().flatten(), bins=100)
                plt.show()
                plt.close()

    def _monitor_gpu_memory(self, step_name="", log_memory=False):
        """ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not torch.cuda.is_available() or not is_main_process():
            return
            
        if log_memory and self.logger:
            allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            cached = torch.cuda.memory_reserved(self.device) / 1024**3
            max_allocated = torch.cuda.max_memory_allocated(self.device) / 1024**3
            
            self.logger.debug(f'{safe_emoji("ğŸ“Š", "[MEMORY]")} {step_name} - '
                            f'å·²åˆ†é…: {allocated:.2f}GB, ç¼“å­˜: {cached:.2f}GB, å³°å€¼: {max_allocated:.2f}GB')
    
    def _optimize_gpu_settings(self):
        """ä¼˜åŒ–GPUè®¾ç½®ä»¥æå‡æ€§èƒ½"""
        if not torch.cuda.is_available():
            return
            
        # å¯ç”¨Tensor Coreï¼ˆå¦‚æœå¯ç”¨ï¼‰
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # é¢„åˆ†é…GPUå†…å­˜ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
        torch.cuda.empty_cache()
        
        if is_main_process() and self.logger:
            self.logger.info(f'{safe_emoji("ğŸš€", "[GPU]")} GPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®å®Œæˆ')
    
    def _warmup_gpu(self):
        """GPUé¢„çƒ­ï¼Œé¿å…é¦–æ¬¡æ¨ç†çš„æ€§èƒ½æŸå¤±"""
        if not torch.cuda.is_available():
            return
            
        if is_main_process() and self.logger:
            self.logger.info(f'{safe_emoji("ğŸ”¥", "[WARMUP]")} å¼€å§‹GPUé¢„çƒ­...')
        
        # ç”Ÿæˆå‡æ•°æ®è¿›è¡Œé¢„çƒ­
        dummy_noise = torch.randn(4, self.latent_dim, 1, 1, device=self.device)
        dummy_images = torch.randn(4, 3, self.image_size, self.image_size, device=self.device)
        
        # é¢„çƒ­ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨
        with torch.no_grad():
            _ = self.generator(dummy_noise)
            _ = self.discriminator(dummy_images)
        
        # æ¸…ç†é¢„çƒ­æ•°æ®
        del dummy_noise, dummy_images
        torch.cuda.empty_cache()
        
        if is_main_process() and self.logger:
            self.logger.info(f'{safe_emoji("âœ¨", "[WARMUP]")} GPUé¢„çƒ­å®Œæˆ')

    def train(self):

        start_time = time.time()
        if is_main_process() and self.logger:
            self.logger.debug(f'{safe_emoji("ğŸš€", "[START]")} å¼€å§‹è®­ç»ƒçš„æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}')

        # åˆ›å»ºå­˜å‚¨ç”Ÿæˆå›¾ç‰‡çš„ç›®å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if is_main_process() and not os.path.exists(os.path.join(self.train_epoch_dir, 'fake_images_new')):
            os.makedirs(os.path.join(self.train_epoch_dir, 'fake_images_new'))

        real_label = 1.0
        fake_label = 0.0
        fixed_noise = torch.randn(64, self.latent_dim, 1, 1, device=self.device)
        img_list = []
        G_losses = []
        D_losses = []
        iters = 0
        early_stopping_counter = 0

        if is_main_process() and self.logger:
            self.logger.debug(f'{safe_emoji("ğŸ”¥", "")}{"<" * 30}å¼€å§‹è®­ç»ƒ{">" * 30}{safe_emoji("ğŸ”¥", "")}')
        
        # GPUä¼˜åŒ–è®¾ç½®
        self._optimize_gpu_settings()
        
        # GPUé¢„çƒ­
        self._warmup_gpu()
        
        # ç›‘æ§åˆå§‹GPUå†…å­˜
        self._monitor_gpu_memory("è®­ç»ƒå¼€å§‹å‰", log_memory=True)
        
        self.generator.train()
        self.discriminator.train()
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        start_epoch = self._load_models_unified()
        self.start_epoch = start_epoch
        
        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­è®¾ç½®epochï¼ˆç”¨äºDistributedSamplerï¼‰
        if self.is_distributed and hasattr(self.train_dataloader.sampler, 'set_epoch'):
            self.train_dataloader.sampler.set_epoch(start_epoch)
        
        try:
            for epoch in range(start_epoch, self.epochs):
                # ä¸ºåˆ†å¸ƒå¼é‡‡æ ·å™¨è®¾ç½®epoch
                if self.is_distributed and hasattr(self.train_dataloader.sampler, 'set_epoch'):
                    self.train_dataloader.sampler.set_epoch(epoch)
                
                for i, data in enumerate(self.train_dataloader, 0):
                    # æ•°æ®å·²ç»åœ¨GPUä¸Šå¹¶ç»è¿‡é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨
                    real_images, _ = data  # è§£åŒ…æ•°æ®
                    
                    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆé€šå¸¸å·²ç»åœ¨äº†ï¼‰
                    if not real_images.is_cuda and self.device.type == 'cuda':
                        real_images = real_images.to(self.device, non_blocking=True)
                    
                    b_size = real_images.size(0)
                    
                    ############################
                    # (1) è®­ç»ƒåˆ¤åˆ«å™¨ï¼šæœ€å¤§åŒ– log(D(x)) + log(1 - D(G(z)))
                    ###########################
                    self.discriminator.zero_grad()
                    
                    # è®­ç»ƒçœŸå®æ•°æ®
                    label = torch.full((b_size,), real_label, dtype=torch.float, device=self.device)
                    
                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                        output = self.discriminator(real_images).view(-1)
                        errD_real = self.criterion(output, label)
                    
                    # æ··åˆç²¾åº¦è®­ç»ƒçš„æ­£ç¡®æµç¨‹
                    self.scaler.scale(errD_real).backward()
                    D_x = output.mean().item()

                    # è®­ç»ƒç”Ÿæˆçš„å‡æ•°æ®
                    noise = torch.randn(b_size, self.latent_dim, 1, 1, device=self.device, dtype=torch.float32)
                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                        fake = self.generator(noise)
                        label.fill_(fake_label)
                        output = self.discriminator(fake.detach()).view(-1)
                        errD_fake = self.criterion(output, label)
                    
                    self.scaler.scale(errD_fake).backward()
                    D_G_z1 = output.mean().item()
                    errD = errD_real + errD_fake
                    
                    # æ¢¯åº¦ç´¯ç§¯å¤„ç†
                    if (i + 1) % self.gradient_accumulation_steps == 0:
                        self.scaler.step(self.optimizer_D)
                        self.scaler.update()
                        self.discriminator.zero_grad()

                    ############################
                    # (2) è®­ç»ƒç”Ÿæˆå™¨ï¼šæœ€å¤§åŒ– log(D(G(z)))
                    ###########################
                    self.generator.zero_grad()
                    label.fill_(real_label)  # ç”Ÿæˆå™¨å¸Œæœ›åˆ¤åˆ«å™¨è®¤ä¸ºå‡æ•°æ®æ˜¯çœŸçš„
                    
                    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                        output = self.discriminator(fake).view(-1)
                        errG = self.criterion(output, label)
                    
                    self.scaler.scale(errG).backward()
                    D_G_z2 = output.mean().item()

                    # æ¢¯åº¦ç´¯ç§¯å¤„ç†
                    if (i + 1) % self.gradient_accumulation_steps == 0:
                        self.scaler.step(self.optimizer_G)
                        self.scaler.update()
                        self.generator.zero_grad()

                    # åªåœ¨ä¸»è¿›ç¨‹è®°å½•æ—¥å¿—
                    if i % 50 == 0 and is_main_process() and self.logger:
                        self.logger.debug('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                                     % (epoch, self.epochs, i, len(self.train_dataloader),
                                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

                    # åœ¨ä¸»è¿›ç¨‹ä¸­è®°å½•æŸå¤±
                    if is_main_process():
                        G_losses.append(errG.item())
                        D_losses.append(errD.item())

                    if ((iters % 500 == 0) or ((epoch == self.epochs - 1) and (i == len(self.train_dataloader) - 1))) and is_main_process():
                        with torch.no_grad():
                            fake = self.generator(fixed_noise.to(torch.float32)).detach().cpu()
                        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

                    iters += 1

                # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
                if self.is_distributed:
                    dist.barrier()

                #åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæ¯ä¸ªepochç»“æŸæ—¶ï¼Œè°ƒç”¨save_stateæ–¹æ³•æ¥ä¿å­˜çŠ¶æ€ï¼š
                self.save_state(epoch)
                
                # ç›‘æ§æ¯ä¸ªepochç»“æŸæ—¶çš„GPUå†…å­˜ä½¿ç”¨
                if epoch % 10 == 0:  # æ¯10ä¸ªepochç›‘æ§ä¸€æ¬¡å†…å­˜
                    self._monitor_gpu_memory(f"Epoch {epoch} ç»“æŸ", log_memory=True)
                
                # å®šæœŸæ¸…ç†GPUç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜ç¢ç‰‡
                if epoch % 20 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # åœ¨æ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜ç”Ÿæˆçš„å›¾åƒï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
                if is_main_process():
                    with torch.no_grad():
                        fake = self.generator(fixed_noise).detach().cpu()
                    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))
                    save_image(fake.data,os.path.join(self.train_epoch_dir, f'fake_images_new/fake_images_epoch_{epoch}_Loss_D_{errD.item():.4f}_Loss_G_{errG.item():.4f}_D_x_{D_x}_D_x_{(D_G_z1/D_G_z2):.4f}.png'), normalize=True)

                # åœ¨è®­ç»ƒå¾ªç¯ç»“æŸåï¼Œæ·»åŠ ä»¥ä¸‹ä»£ç ä»¥æ‰“å°ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„æŸå¤±å¹¶å¯è§†åŒ–ç”Ÿæˆçš„å›¾åƒ
                self.generator.eval()
                self.discriminator.eval()

                if epoch % 50==0 and is_main_process():
                    # å°†ç”Ÿæˆçš„å›¾åƒæ˜¾ç¤ºåœ¨æ§åˆ¶å°
                    plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True).cpu(), (1, 2, 0)))
                    plt.axis('off')
                    #plt.show()

                    # ç»˜åˆ¶ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„æŸå¤±æ›²çº¿
                    plt.figure(figsize=(10, 5))
                    plt.title(f"Generator and Discriminator Loss After Epoch {epoch}")
                    plt.plot(G_losses, label="Generator Loss")
                    plt.plot(D_losses, label="Discriminator Loss")
                    plt.xlabel("Iterations")
                    plt.ylabel("Loss")
                    plt.legend()
                    #plt.show()

                # ä¿®æ­£éªŒè¯é€»è¾‘ï¼šä½¿ç”¨validation_frequencyè€Œä¸æ˜¯patienceä½œä¸ºéªŒè¯é¢‘ç‡
                if epoch % self.validation_frequency == 0:
                    self.generator.eval()
                    val_loss = 0
                    val_batches = 0
                    
                    with torch.no_grad():
                        for i, data in enumerate(self.val_dataloader, 0):
                            # æ•°æ®å·²ç»åœ¨GPUä¸Šå¹¶ç»è¿‡é¢„å¤„ç†
                            real_images, _ = data
                            
                            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                            if not real_images.is_cuda and self.device.type == 'cuda':
                                real_images = real_images.to(self.device, non_blocking=True)
                            
                            b_size = real_images.size(0)
                            label = torch.full((b_size,), real_label, dtype=torch.float, device=self.device)
                            
                            # éªŒè¯åˆ¤åˆ«å™¨å¯¹çœŸå®æ•°æ®çš„è¡¨ç°
                            output = self.discriminator(real_images).view(-1)
                            errD_real = self.criterion(output, label)
                            D_x = output.mean().item()

                            # éªŒè¯åˆ¤åˆ«å™¨å¯¹ç”Ÿæˆæ•°æ®çš„è¡¨ç°
                            noise = torch.randn(b_size, self.latent_dim, 1, 1, device=self.device, dtype=torch.float32)
                            fake = self.generator(noise)
                            label.fill_(fake_label)
                            output = self.discriminator(fake.detach()).view(-1)
                            errD_fake = self.criterion(output, label)
                            D_G_z1 = output.mean().item()
                            errD = errD_real + errD_fake

                            # éªŒè¯ç”Ÿæˆå™¨è¡¨ç°
                            label.fill_(real_label)
                            output = self.discriminator(fake).view(-1)
                            errG = self.criterion(output, label)
                            D_G_z2 = output.mean().item()

                            val_loss += errG.item() + errD.item()
                            val_batches += 1

                    val_loss /= val_batches if val_batches > 0 else 1
                    
                    if is_main_process() and self.logger:
                        self.logger.debug(f'è®­ç»ƒåˆ°ç¬¬{epoch}ä¸ªå‘¨æœŸåçš„éªŒè¯æŸå¤±ä¸º: {val_loss:.6f}')

                    # æ—©åœé€»è¾‘ï¼šå¦‚æœéªŒè¯æŸå¤±æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
                    if val_loss < self.best_val_loss - self.min_delta:
                        self.best_val_loss = val_loss
                        early_stopping_counter = 0

                        self.generator.train()
                        self.discriminator.train()

                        # åœ¨æ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜æ­¤æ—¶çš„æ¨¡å‹æ–‡ä»¶ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
                        if is_main_process():
                            # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå‰¥ç¦»DDPåŒ…è£…ï¼‰
                            generator_model = self.generator.module if self.is_distributed else self.generator
                            discriminator_model = self.discriminator.module if self.is_distributed else self.discriminator
                            
                            # ä¿å­˜ç”Ÿæˆå™¨çš„æ¨¡å‹
                            torch.save({
                                'epoch': epoch,
                                'generator_state_dict': generator_model.state_dict(),
                                'optimizer_G_state_dict': self.optimizer_G.state_dict(),
                                'scheduler_G_state_dict': self.scheduler_G.state_dict(),
                                'best_val_loss': self.best_val_loss,
                                'val_loss': val_loss,
                                'early_stopping_counter': early_stopping_counter,
                            },os.path.join(self.train_epoch_dir, f'generator_all_epoch_{start_epoch}to{self.epochs}.pth'))

                            # ä¿å­˜åˆ¤åˆ«å™¨çš„æ¨¡å‹
                            torch.save({
                                'epoch': epoch,
                                'discriminator_state_dict': discriminator_model.state_dict(),
                                'optimizer_D_state_dict': self.optimizer_D.state_dict(),
                                'scheduler_D_state_dict': self.scheduler_D.state_dict(),
                                'best_val_loss': self.best_val_loss,
                                'val_loss': val_loss,
                                'early_stopping_counter': early_stopping_counter,
                            },os.path.join(self.train_epoch_dir, f'discriminator_all_epoch_{start_epoch}to{self.epochs}.pth'))

                    else:
                        early_stopping_counter += 1
                        if is_main_process() and self.logger:
                            self.logger.debug(f'è®­ç»ƒåˆ°ç¬¬{epoch}ä¸ªå‘¨æœŸåæ—©åœæ¬¡æ•°+1ï¼Œå½“å‰æ—©åœæ¬¡æ•°ä¸ºï¼š{early_stopping_counter}.å› ä¸ºéªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„ï¼Œæˆ–è€…æ”¹å–„çš„å¹…åº¦å°äºmin_delta')
                        if early_stopping_counter >= self.patience:
                            if is_main_process() and self.logger:
                                self.logger.debug(
                                    f'è®­ç»ƒåˆ°ç¬¬{epoch}ä¸ªå‘¨æœŸååœæ­¢è®­ç»ƒï¼Œå› ä¸ºæ—©åœæ¬¡æ•°å¤§äºç­‰äºè€å¿ƒå€¼ï¼š{self.patience}ä¸”éªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„ï¼Œæˆ–è€…æ”¹å–„çš„å¹…åº¦å°äºmin_delta')
                            break

        except KeyboardInterrupt:
            self.logger.info(f'\n{safe_emoji("ğŸ›‘", "[STOP]")} ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒ (Epoch {epoch}) {safe_emoji("ğŸ›‘", "[STOP]")}')
            self.logger.info(f'{safe_emoji("ğŸ’¾", "[SAVE]")} æ­£åœ¨ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€...')
            # ä¿å­˜å½“å‰çŠ¶æ€
            self.save_state(epoch)
            # ä¿å­˜ç®€å•æ¨¡å‹
            torch.save(self.generator.state_dict(), os.path.join(self.train_epoch_dir,f'generator_simple_epoch_{start_epoch}to{epoch}_interrupted.pth'))
            torch.save(self.discriminator.state_dict(), os.path.join(self.train_epoch_dir,f'discriminator_simple_epoch_{start_epoch}to{epoch}_interrupted.pth'))
            self.logger.info(f'{safe_emoji("âœ…", "[SUCCESS]")} è®­ç»ƒçŠ¶æ€å·²ä¿å­˜ï¼å¯ä»¥é€šè¿‡è®¾ç½®start_epoch={epoch+1}æ¥æ¢å¤è®­ç»ƒ')
            return G_losses, D_losses, img_list

        self.logger.debug(f'{safe_emoji("ğŸ‰", "[COMPLETE]")} è®­ç»ƒç»“æŸï¼')

        # ä¿å­˜æ¨¡å‹
        torch.save(self.generator.state_dict(), os.path.join(self.train_epoch_dir,f'generator_simple_epoch_{start_epoch}to{self.epochs}.pth'))
        torch.save(self.discriminator.state_dict(), os.path.join(self.train_epoch_dir,f'discriminator_simple_epoch_{start_epoch}to{self.epochs}.pth'))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.figure(figsize=(10, 5))
        plt.title(f"Generator and Discriminator Loss During Training Between {start_epoch}To{self.epochs} Epoch \nè®­ç»ƒæœŸé—´ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æŸè€—")
        plt.plot(G_losses, label="Gç”Ÿæˆå™¨")
        plt.plot(D_losses, label="Dåˆ¤åˆ«å™¨")
        plt.xlabel("iterationsè¿­ä»£")
        plt.ylabel("LossæŸè€—")
        plt.legend()
        plt.show()

        # åŠ¨æ€å±•ç¤ºç”Ÿæˆå›¾ç‰‡çš„è¿‡ç¨‹
        # åœ¨åˆ›å»ºåŠ¨ç”»ä¹‹å‰è®¾ç½®embed_limit
        plt.rcParams['animation.embed_limit'] = 30.0  # æˆ–è€…è®¾ç½®é€‚åˆä½ éœ€æ±‚çš„å€¼

        fig = plt.figure(figsize=(8, 8))
        plt.axis("off")
        ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]
        ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)
        HTML(ani.to_jshtml())

        #å¦å¤–ä¿å­˜ç”Ÿæˆå›¾ç‰‡çš„è¿‡ç¨‹åŠ¨ç”»ä¸ºhtmlæ–‡ä»¶

        '''
        å®ƒé¦–å…ˆä½¿ç”¨to_jshtmlæ–¹æ³•å°†åŠ¨ç”»è½¬æ¢ä¸ºä¸€ä¸ªHTMLå­—ç¬¦ä¸²ï¼Œç„¶åå°†è¿™ä¸ªå­—ç¬¦ä¸²å†™å…¥åˆ°ä¸€ä¸ªåä¸ºanimation.htmlçš„æ–‡ä»¶ä¸­ã€‚
        ä½ å¯ä»¥åœ¨ä»»ä½•æµè§ˆå™¨ä¸­æ‰“å¼€è¿™ä¸ªæ–‡ä»¶æ¥æŸ¥çœ‹åŠ¨ç”»ã€‚
        '''

        html = ani.to_jshtml()
        with open(os.path.join(self.train_epoch_dir,f'animation_epoch_{self.start_epoch}to{self.epochs}.html'), 'w') as f:

            f.write(html)

        # å¯è§†åŒ–æ¨¡å‹è¾“å‡º
        self.visualize_model_output()
        # å¯è§†åŒ–æƒé‡
        self.visualize_weights()

        self.logger.debug(f'{safe_emoji("ğŸ”¥", "")}{"<" * 30}è®­ç»ƒç»“æŸ{">" * 30}{safe_emoji("ğŸ”¥", "")}')

        # è®¡ç®—è€—æ—¶
        end_time = time.time()
        self.logger.debug(f'{safe_emoji("ğŸ", "[END]")} ç»“æŸè®­ç»ƒçš„æ—¶é—´ä¸ºï¼š{time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}')
        diff_time = end_time - start_time
        hours = int(diff_time // 3600)
        minutes = int((diff_time % 3600) // 60)
        seconds = int(diff_time % 60)
        self.logger.debug(f'{safe_emoji("â±ï¸", "[TIME]")} è®­ç»ƒæ€»è€—æ—¶ï¼š{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’')
        return G_losses, D_losses, img_list


if __name__ == '__main__':
    # ä¿®æ”¹ç¡¬ç¼–ç è·¯å¾„ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ä»ç¯å¢ƒå˜é‡è·å–
    dataset_path = os.getenv('DATASET_PATH', './dataset')  # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œé»˜è®¤ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    trainer = GANTrainer(
        dataset_path=dataset_path, 
        num_layers=4, 
        base_channels=64,
        load_models=True, 
        epochs=2000,
        batch_size=256,
        patience=500,
        validation_frequency=10  # æ–°å¢å‚æ•°ï¼šæ¯10ä¸ªepochéªŒè¯ä¸€æ¬¡
    )
    trainer.train()
